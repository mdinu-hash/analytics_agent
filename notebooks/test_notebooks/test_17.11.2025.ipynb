{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir,\"..\"))\n",
    "project_root = os.path.abspath(os.path.join(project_root,\"..\")) # one level more down\n",
    "paths_to_add = [project_root,\n",
    "                os.path.join(project_root,\"src\",\"init\")]\n",
    "for path in paths_to_add:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
    "# connection to db\n",
    "connection_string = os.getenv('CONNECTION_STRING_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All terms in synonyms and related_terms exist in key_terms\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "graph = agent.graph\n",
    "\n",
    "orchestrator = agent.orchestrator\n",
    "run_control_flow = agent.run_control_flow\n",
    "generate_answer = agent.generate_answer\n",
    "create_sql_query_or_queries = agent.create_sql_query_or_queries\n",
    "extract_analytical_intent = agent.extract_analytical_intent\n",
    "\n",
    "# Import initialization components\n",
    "from src.init.initialization import (\n",
    "    llm, llm_fast, create_config, tracer,\n",
    "    objects_documentation, sql_dialect, connection_string\n",
    ")\n",
    "\n",
    "question = 'placeholder'\n",
    "test_state = {\n",
    "'objects_documentation':objects_documentation,\n",
    "'sql_dialect': sql_dialect,\n",
    "'messages_log':[],\n",
    "'intermediate_steps' : [],\n",
    "'analytical_intent': [],\n",
    "'current_question':question,\n",
    "'current_sql_queries': [],\n",
    "'generate_answer_details': {},\n",
    "'llm_answer': AIMessage(content='')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the conversation with the graph\n",
    "question = 'What is the distribution of assets per household?' \n",
    "\n",
    "test_state['current_question'] = question\n",
    "vector_store = None  # reset vector store\n",
    "config, thread_id = create_config('Run Agent',True)\n",
    "\n",
    "result = graph.invoke(test_state, config = config)\n",
    "display = f'''Analytical intent: {result['analytical_intent']}\\n\\nSQL query: {result['current_sql_queries']}\\n\\nGenerate Answer Details: {result['generate_answer_details']}\\n\\nAnswer: {result['llm_answer'].content}'''\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the conversation with the graph (followup 1)\n",
    "question = 'can you say again which one is the best one? sorry i missed it'\n",
    "\n",
    "test_state['current_question'] = question\n",
    "vector_store = None  # reset vector store\n",
    "config, _ = create_config('Run Agent', False, thread_id) # (re-use same thread)\n",
    "result = graph.invoke(test_state, config)\n",
    "display = f'''Analytical intent: {result['analytical_intent']}\\n\\nSQL query: {result['current_sql_queries']}\\n\\nGenerate Answer Details: {result['generate_answer_details']}\\n\\nAnswer: {result['llm_answer']}'''\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "orchestrator = agent.orchestrator\n",
    "run_control_flow = agent.run_control_flow\n",
    "generate_answer = agent.generate_answer\n",
    "create_sql_query_or_queries = agent.create_sql_query_or_queries\n",
    "extract_analytical_intent = agent.extract_analytical_intent\n",
    "\n",
    "# Import initialization components\n",
    "from src.init.initialization import  llm, llm_fast, create_config, tracer, objects_documentation, sql_dialect, connection_string\n",
    "\n",
    "question = 'What is the distribution of assets per household?'\n",
    "\n",
    "test_state = {\n",
    "'objects_documentation':objects_documentation,\n",
    "'sql_dialect': sql_dialect,\n",
    "'messages_log':[],\n",
    "'intermediate_steps' : [],\n",
    "'analytical_intent': [],\n",
    "'current_question':question,\n",
    "'current_sql_queries': [],\n",
    "'generate_answer_details': {},\n",
    "'llm_answer': AIMessage(content='')\n",
    "}\n",
    "\n",
    "#orchestrator(test_state)\n",
    "#test_state = run_control_flow(test_state) # extract_analytical_intent\n",
    "#test_state = run_control_flow(test_state) # create sql query + execute sql query\n",
    "#orchestrator(test_state)\n",
    "#test_state = run_control_flow(test_state) # generate answer + manage memory\n",
    "# test_state = generate_answer.invoke({'state':test_state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import create_prompt_template\n",
    "from typing_extensions import TypedDict, Annotated, Literal\n",
    "\n",
    "def extract_msg_content_from_history(messages_log:list):\n",
    " ''' from a list of base messages, extract just the content '''\n",
    " content = []\n",
    " for msg in messages_log:\n",
    "     content.append(msg.content)\n",
    " return \"\\n\".join(content)\n",
    "\n",
    "\n",
    "def format_sql_query_results_for_prompt (sql_queries : list[dict]) -> str:\n",
    "    \"\"\" based on the current_sql_queries, creates a string like so: Insight 1: ... Raw Result of insight 1: ... Insight 2 ... etc \"\"\"\n",
    "    formatted_queries = []\n",
    "    for query_index,q in enumerate(sql_queries):\n",
    "        block = f\"Insight {query_index+1}:\\n{q['insight']}\\n\\nRaw Result of insight {query_index+1}:\\n{q['result']}\"\n",
    "        formatted_queries.append(block)\n",
    "    return \"\\n\\n\".join(formatted_queries)\n",
    "\n",
    "class ScenarioBC(TypedDict):\n",
    "  ''' indication of the next step to be performed by the agent '''\n",
    "  next_step: Annotated[Literal[\"B\", \"C\",\"Continue\"],\"indication of the next step to be performed by the agent\"] \n",
    "\n",
    "system_prompt = f\"\"\"You are a decision support consultant helping users make data-driven decisions.\n",
    "\n",
    "    Your task is to decide the next action for this question: {{question}}.\n",
    "\n",
    "    Conversation history: {{messages_log}}. \n",
    "    Current insights: \"{{insights}}\".\n",
    "    Database schema: {{objects_documentation}}\n",
    "\n",
    "    Decision process:  \n",
    "\n",
    "    Step 1. Check if question is non-analytical or already answered:\n",
    "       - If question is just pleasantries (\"thank you\", \"hello\", \"how are you\") → \"B\"\n",
    "       - If the same question was already answered in conversation history → \"B\"\n",
    "\n",
    "    Step 2. Check if requested data exists in schema:\n",
    "      - If the user asks for data/metrics not available in the database schema → \"C\"\n",
    "    \n",
    "    Step 3. Otherwise → \"Continue\".\n",
    "    \"\"\"\n",
    "prompt = create_prompt_template('system', system_prompt)\n",
    "chain = prompt | llm_fast.with_structured_output(ScenarioBC)\n",
    "result = chain.invoke({'messages_log':extract_msg_content_from_history(test_state['messages_log']),\n",
    "                         'question': test_state['current_question'], \n",
    "                         'insights': format_sql_query_results_for_prompt(test_state['current_sql_queries']),\n",
    "                         'objects_documentation':test_state['objects_documentation']\n",
    "                         })   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a0fce415d2e9e6477a25b7ed122e649823ddf5df417fe3f69c00ade6d8eb08b"
  },
  "kernelspec": {
   "display_name": "Python 3.13.5 ('.venv-py313': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
