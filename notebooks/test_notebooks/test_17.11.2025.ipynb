{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir,\"..\"))\n",
    "project_root = os.path.abspath(os.path.join(project_root,\"..\")) # one level more down\n",
    "paths_to_add = [project_root,\n",
    "                os.path.join(project_root,\"src\",\"init\")]\n",
    "for path in paths_to_add:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
    "# connection to db\n",
    "connection_string = os.getenv('CONNECTION_STRING_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All terms in synonyms and related_terms exist in key_terms\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "graph = agent.graph\n",
    "\n",
    "orchestrator = agent.orchestrator\n",
    "run_control_flow = agent.run_control_flow\n",
    "generate_answer = agent.generate_answer\n",
    "create_sql_query_or_queries = agent.create_sql_query_or_queries\n",
    "extract_analytical_intent = agent.extract_analytical_intent\n",
    "\n",
    "# Import initialization components\n",
    "from src.init.initialization import (\n",
    "    llm, llm_fast, create_config, tracer,\n",
    "    objects_documentation, sql_dialect, connection_string\n",
    ")\n",
    "\n",
    "question = 'placeholder'\n",
    "test_state = {\n",
    "'objects_documentation':objects_documentation,\n",
    "'sql_dialect': sql_dialect,\n",
    "'messages_log':[],\n",
    "'intermediate_steps' : [],\n",
    "'analytical_intent': [],\n",
    "'current_question':question,\n",
    "'current_sql_queries': [],\n",
    "'generate_answer_details': {\n",
    "    'key_assumptions': [],\n",
    "    'agent_questions': [],\n",
    "    'ambiguity_explanation': ''\n",
    "},\n",
    "'llm_answer': AIMessage(content=''),\n",
    "'scenario': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the conversation with the graph\n",
    "question = 'What is the distribution of assets per household across asset ranges?' \n",
    "\n",
    "test_state['current_question'] = question\n",
    "vector_store = None  # reset vector store\n",
    "config, thread_id = create_config('Run Agent',True)\n",
    "\n",
    "result = graph.invoke(test_state, config = config)\n",
    "display = f'''Analytical intent: {result['analytical_intent']}\\n\\nSQL query: {result['current_sql_queries']}\\n\\nGenerate Answer Details: {result['generate_answer_details']}\\n\\nAnswer: {result['llm_answer'].content}'''\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the conversation with the graph (followup 1)\n",
    "question = 'can you say again which one is the best one? sorry i missed it'\n",
    "\n",
    "test_state['current_question'] = question\n",
    "vector_store = None  # reset vector store\n",
    "config, _ = create_config('Run Agent', False, thread_id) # (re-use same thread)\n",
    "result = graph.invoke(test_state, config)\n",
    "display = f'''Analytical intent: {result['analytical_intent']}\\n\\nSQL query: {result['current_sql_queries']}\\n\\nGenerate Answer Details: {result['generate_answer_details']}\\n\\nAnswer: {result['llm_answer']}'''\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "orchestrator = agent.orchestrator\n",
    "run_control_flow = agent.run_control_flow\n",
    "generate_answer = agent.generate_answer\n",
    "create_sql_query_or_queries = agent.create_sql_query_or_queries\n",
    "extract_analytical_intent = agent.extract_analytical_intent\n",
    "\n",
    "# Import initialization components\n",
    "from src.init.initialization import  llm, llm_fast, create_config, tracer, objects_documentation, sql_dialect, connection_string\n",
    "\n",
    "question = 'What is the distribution of assets per household?'\n",
    "\n",
    "test_state = {\n",
    "'objects_documentation':objects_documentation,\n",
    "'sql_dialect': sql_dialect,\n",
    "'messages_log':[],\n",
    "'intermediate_steps' : [],\n",
    "'analytical_intent': [],\n",
    "'current_question':question,\n",
    "'current_sql_queries': [],\n",
    "'generate_answer_details': {\n",
    "    'key_assumptions': [],\n",
    "    'agent_questions': [],\n",
    "    'ambiguity_explanation': ''\n",
    "},\n",
    "'llm_answer': AIMessage(content=''),\n",
    "'scenario': ''\n",
    "}\n",
    "\n",
    "#orchestrator(test_state)\n",
    "#test_state = run_control_flow(test_state) # extract_analytical_intent\n",
    "#test_state = run_control_flow(test_state) # create sql query + execute sql query\n",
    "#orchestrator(test_state)\n",
    "#test_state = run_control_flow(test_state) # generate answer + manage memory\n",
    "# test_state = generate_answer.invoke({'state':test_state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for extract_analytical_intent\nstate.scenario\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_state = \u001b[43mrun_control_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_state\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GDrive\\Github\\analytics_agent\\agent.py:1014\u001b[39m, in \u001b[36mrun_control_flow\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m   1012\u001b[39m \u001b[38;5;66;03m# extract_analytical_intent\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_name == \u001b[33m'\u001b[39m\u001b[33mextract_analytical_intent\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m   state = \u001b[43mextract_analytical_intent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# creating & executing new queries\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m tool_name == \u001b[33m'\u001b[39m\u001b[33mcreate_sql_query_or_queries\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GDrive\\Github\\analytics_agent\\.venv-py313\\Lib\\site-packages\\langchain_core\\tools\\base.py:513\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    507\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs: Any,\n\u001b[32m    511\u001b[39m ) -> Any:\n\u001b[32m    512\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GDrive\\Github\\analytics_agent\\.venv-py313\\Lib\\site-packages\\langchain_core\\tools\\base.py:774\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    773\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    775\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    776\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GDrive\\Github\\analytics_agent\\.venv-py313\\Lib\\site-packages\\langchain_core\\tools\\base.py:736\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    734\u001b[39m child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     tool_args, tool_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_args_and_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m signature(\u001b[38;5;28mself\u001b[39m._run).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    740\u001b[39m         tool_kwargs = tool_kwargs | {\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m: run_manager}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GDrive\\Github\\analytics_agent\\.venv-py313\\Lib\\site-packages\\langchain_core\\tools\\base.py:651\u001b[39m, in \u001b[36mBaseTool._to_args_and_kwargs\u001b[39m\u001b[34m(self, tool_input, tool_call_id)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    644\u001b[39m     \u001b[38;5;28mself\u001b[39m.args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    645\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.args_schema, \u001b[38;5;28mtype\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m ):\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# StructuredTool with no args\u001b[39;00m\n\u001b[32m    650\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (), {}\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m tool_input = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[38;5;66;03m# For backwards compatibility, if run_input is a string,\u001b[39;00m\n\u001b[32m    653\u001b[39m \u001b[38;5;66;03m# pass as a positional argument.\u001b[39;00m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_input, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GDrive\\Github\\analytics_agent\\.venv-py313\\Lib\\site-packages\\langchain_core\\tools\\base.py:570\u001b[39m, in \u001b[36mBaseTool._parse_input\u001b[39m\u001b[34m(self, tool_input, tool_call_id)\u001b[39m\n\u001b[32m    568\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    569\u001b[39m             tool_input[k] = tool_call_id\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     result = \u001b[43minput_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m     result_dict = result.model_dump()\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(input_args, BaseModelV1):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GDrive\\Github\\analytics_agent\\.venv-py313\\Lib\\site-packages\\pydantic\\main.py:703\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    699\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    700\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    701\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for extract_analytical_intent\nstate.scenario\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type"
     ]
    }
   ],
   "source": [
    "test_state = run_control_flow(test_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.agents import AgentAction\n",
    "from typing_extensions import Annotated,Literal,Union,TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm_provider = 'anthropic'  # Change to 'openai' or 'anthropic' as needed\n",
    "\n",
    "def extract_msg_content_from_history(messages_log:list):\n",
    " ''' from a list of base messages, extract just the content '''\n",
    " content = []\n",
    " for msg in messages_log:\n",
    "     content.append(msg.content)\n",
    " return \"\\n\".join(content)\n",
    "\n",
    "class ClearOrAmbiguous(TypedDict):\n",
    "  ''' conclusion about the analytical intent extraction process '''\n",
    "  analytical_intent_clearness: Annotated[Literal[\"Analytical Intent Extracted\", \"Analytical Intent Ambiguous\"],\"conclusion about the analytical intent extraction process\"] \n",
    "\n",
    "class AnalyticalIntents(TypedDict):\n",
    "  ''' list of analytical intents '''\n",
    "  analytical_intent: Annotated[Union[list[str], None] ,\"natural language descriptions to capture the analytical intents\"]\n",
    "\n",
    "class AmbiguityAnalysis(TypedDict):\n",
    "  ''' analysis of ambiguous question with explanation and alternatives '''\n",
    "  ambiguity_explanation: Annotated[str, \"brief explanation of what makes the question ambiguous\"]\n",
    "  agent_questions: Annotated[list[str], \"2-3 alternative analytical intents as questions\"]     \n",
    "\n",
    "sys_prompt_clear_or_ambiguous = \"\"\"Decide whether the user question is clear or ambigous based on this specific database schema:\n",
    "{objects_documentation}.\n",
    "\n",
    "Conversation history:\n",
    "\"{messages_log}\".\n",
    "\n",
    "User question:\n",
    "\"{question}\".\n",
    "\n",
    "*** The question is CLEAR if ***\n",
    "- It has a single, obvious analytical approach in terms of underlying source columns, relationships or past conversations.    \n",
    "  Example: \"what is the revenue?\" is clear in a database schema that contains just 1 single metric that can answer the question (ex: net_revenue).\n",
    "\n",
    "- The column and metric naming in the schema clearly points to one dominant method of interpretation. \n",
    "  Example: \"what is the top client?\" is clear in a database schema that contains just 1 single metric that can answer the question (ex: sales_amount). \n",
    "\n",
    "- You can apply reasonable assumptions. Examples:\n",
    "  No specific time periods indicated -> assume a recent period -> CLEAR.\n",
    "  No level of details specified -> use highest aggregation level -> CLEAR.\n",
    "\n",
    "- You can deduct the analytical intent from the conversation history.\n",
    "\n",
    "*** The question is AMBIGUOUS if ***\n",
    "- Different source columns would give different insights.     \n",
    "\n",
    "- Different metrics could answer the same question:\n",
    "  Example: \"What is the top client?\" is ambigous in a database schema that contains multiple metrics that can answer the question (highest value of sales / highest number of sales). \n",
    "\n",
    "Response format:\n",
    "If CLEAR -> \"Analytical Intent Extracted\".\n",
    "If AMBIGUOUS -> \"Analytical Intent Ambiguous\". \n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_clear = \"\"\"Refine technically the user ask for a sql developer with access to the following database schema:\n",
    "{objects_documentation}.\n",
    "\n",
    "Conversation history:\n",
    "\"{messages_log}\".\n",
    "\n",
    "Last user prompt:\n",
    "\"{question}\".\n",
    "\n",
    "Important considerations about creating analytical intents:\n",
    "    - The analytical intent will be used to create a single sql query.\n",
    "    - Write it in 1 sentence.\n",
    "    - Mention just the column names, tables names, grouping levels, aggregation functions (preffered if it doesn't restrict insights) and filters from the database schema.\n",
    "    - If the user ask is exploratory (ex: \"What can you tell me about the dataset?\"), create 3-5 analytical intents.\n",
    "    - If the user ask is non-exploratory, create only one analytical intent.\n",
    "    - If the user asks for statistical analysis between variables (ex correlation) do not compute the statistical metrics, instead just show a simple side by side or group summary.\n",
    "\n",
    "Important considerations about time based analysis:\n",
    "  - If the source columns are from tables showing evolutions of metrics over time, clearly specify the time range to apply the analysis on:\n",
    "    Example: If the question does not specify a time range, specify a recent period like last 12 months, last 3 months.\n",
    "  - Use explicit date filters instead of relative expressions like \"last 12 months\". \n",
    "  - Derive actual date ranges from the database schema under \"Important considerations about dates available\".\n",
    "  - Group the specified period in time-based groups (monthly, quarterly) and compare first with last group.\n",
    "\n",
    "Important considerations about complex, multi-steps analytical intents:\n",
    "- An analytical query is multi-step if it requires sequential data gathering and statistical analysis,\n",
    "  where each search builds upon previous results to examine relationships, correlations, or comparative patterns between variables.\n",
    "- Break it down into sequential steps where each represents a distinct analytical intent:\n",
    "  Template: \"Step 1: <analytical intent 1>. Step 2: <analytical intent 2>. Step 3: <analytical intent 3>\"\n",
    "  \"\"\"\n",
    "\n",
    "sys_prompt_ambiguous = \"\"\"\n",
    "The latest user question is ambiguous based on the following database schema:\n",
    "{objects_documentation}.\n",
    "\n",
    "Here is the conversation history with the user:\n",
    "\"{messages_log}\".\n",
    "\n",
    "Latest user message:\n",
    "\"{question}\".\n",
    "\n",
    "Step 1: Identify what makes the question ambiguous. The question is ambiguous if:\n",
    "\n",
    "- Different source columns would give substantially different insights:\n",
    "  Example: pre-aggregated vs computed metrics with different business logic.\n",
    "\n",
    "- Multiple fundamentally different metrics could answer the same question:\n",
    "  Example: \"What is the top client?\" is ambiguous in a database schema that contains multiple metrics that can answer the question (highest value of sales / highest number of sales).\n",
    "\n",
    "- Different columns with the same underlying source data (check database schema) do NOT create ambiguity.\n",
    "\n",
    "Step 2: Create maximum 3 alternatives of analytical intents to choose from.\n",
    "    - Do not include redundant intents, be focused.\n",
    "    - Each analytical intent is for creating one single sql query.\n",
    "    - Write each analytical intent using 1 sentence.\n",
    "    - Mention specific column names, tables names, aggregation functions and filters from the database schema.\n",
    "    - Mention only the useful info for creating sql queries.\n",
    "\n",
    "Step 3: Create a brief explanation in this format:\n",
    "  1. One sentence explaining the ambiguity\n",
    "  2. Present the 2-3 alternatives as clear options for the user to choose from\n",
    "\n",
    "Use simple, non-technical language. Be concise.\n",
    "\"\"\"  \n",
    "\n",
    "prompt_clear_or_ambiguous = create_prompt_template('system', sys_prompt_clear_or_ambiguous)\n",
    "chain_1= prompt_clear_or_ambiguous | llm.with_structured_output(ClearOrAmbiguous)  \n",
    "\n",
    "prompt_clear = create_prompt_template('system', sys_prompt_clear)\n",
    "chain_2= prompt_clear | llm.with_structured_output(AnalyticalIntents)\n",
    "\n",
    "prompt_ambiguous = create_prompt_template('system', sys_prompt_ambiguous)\n",
    "chain_3= prompt_ambiguous | llm.with_structured_output(AmbiguityAnalysis)\n",
    "\n",
    "# Prepare common input data\n",
    "input_data = {\n",
    "      'objects_documentation': test_state['objects_documentation'], \n",
    "      'question': test_state['current_question'], \n",
    "      'messages_log': extract_msg_content_from_history(test_state['messages_log'])\n",
    " }\n",
    "\n",
    "# determine if clear or ambiguous\n",
    "result_1 = chain_1.invoke(input_data)\n",
    "\n",
    "# Based on result, invoke appropriate chain\n",
    "if result_1['analytical_intent_clearness'] == \"Analytical Intent Extracted\":\n",
    "      # create analytical intents\n",
    "      result_2 = chain_2.invoke(input_data)\n",
    "      # next tool to call \n",
    "      tool_name = 'create_sql_query_or_queries' \n",
    "      output = {\n",
    "          'scenario': 'A',\n",
    "          'analytical_intent': result_2['analytical_intent'],\n",
    "          'agent_questions': None\n",
    "      }\n",
    "elif result_1['analytical_intent_clearness'] == \"Analytical Intent Ambiguous\":\n",
    "       # create ambiguity analysis (combines both analytical intents and explanation)\n",
    "       result_3 = chain_3.invoke(input_data)\n",
    "       # next tool to call\n",
    "       tool_name = 'generate_answer'\n",
    "       output = {\n",
    "          'scenario': 'D',\n",
    "          'analytical_intent': result_3['agent_questions'],\n",
    "          'agent_questions': result_3['agent_questions'] }\n",
    "\n",
    "       # store in generate_answer_details\n",
    "       test_state['generate_answer_details']['ambiguity_explanation'] = result_3['ambiguity_explanation']\n",
    "       test_state['generate_answer_details']['agent_questions'] = result_3['agent_questions']\n",
    "\n",
    "# update the state\n",
    "test_state['scenario'] = output['scenario']\n",
    "test_state['generate_answer_details']['agent_questions'] = output['agent_questions']\n",
    "test_state['analytical_intent'] = output['analytical_intent']\n",
    "\n",
    "# control flow\n",
    "action = AgentAction(tool='extract_analytical_intent', tool_input='',log='tool ran successfully')\n",
    "test_state['intermediate_steps'].append(action)\n",
    "test_state['intermediate_steps'].append(AgentAction(tool=tool_name, tool_input='',log=''))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a0fce415d2e9e6477a25b7ed122e649823ddf5df417fe3f69c00ade6d8eb08b"
  },
  "kernelspec": {
   "display_name": "Python 3.13.5 ('.venv-py313': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
