{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add root to sys path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "root = os.path.abspath(os.path.join(current_dir,\"..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model='claude-sonnet-4-20250514', temperature=0)  # Smart & expensive'''\n",
    "\n",
    "answer = llm.invoke({'question':'hello'})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test in isolation the agent components step by step\n",
    "\n",
    "import agent\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "graph = agent.graph\n",
    "create_config = agent.create_config\n",
    "objects_documentation = agent.objects_documentation\n",
    "sql_dialect = agent.sql_dialect\n",
    "orchestrator = agent.orchestrator\n",
    "run_control_flow = agent.run_control_flow\n",
    "generate_answer = agent.generate_answer\n",
    "\n",
    "# Import initialization components\n",
    "from src.init.initialization import (\n",
    "    llm, llm_fast, create_config, tracer,\n",
    "    objects_documentation, sql_dialect, connection_string\n",
    ")\n",
    "\n",
    "vector_store = None  # reset vector store\n",
    "question = 'Which practice segment has the highest asset growth rate in the last 12 months?' \n",
    "test_state = {\n",
    "'objects_documentation':objects_documentation,\n",
    "'sql_dialect': sql_dialect,\n",
    "'messages_log':[],\n",
    "'intermediate_steps' : [],\n",
    "'analytical_intent': [],\n",
    "'current_question':question,\n",
    "'current_sql_queries': [],\n",
    "'generate_answer_details': {},\n",
    "'llm_answer': AIMessage(content=''),\n",
    "'scenario': '',\n",
    "'search_terms_output':{}\n",
    "}\n",
    "\n",
    "#orchestrator.(test_state)\n",
    "#test_state = run_control_flow(test_state) # extract_analytical_intent\n",
    "#test_state = run_control_flow(test_state) # create sql query + execute sql query\n",
    "#orchestrator(test_state)\n",
    "#test_state = run_control_flow(test_state) # generate answer + manage memory\n",
    "# test_state = generate_answer.invoke({'state':test_state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state = orchestrator(test_state)\n",
    "test_state = run_control_flow(test_state) # clarification_check\n",
    "test_state = run_control_flow(test_state) # extract_analytical_intent\n",
    "test_state = run_control_flow(test_state) # create sql query \n",
    "test_state = run_control_flow(test_state) # execute sql query \n",
    "test_state = run_control_flow(test_state) # generate answer \n",
    "test_state = run_control_flow(test_state) # add_assumptions\n",
    "#test_state = run_control_flow(test_state) # manage_memory_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_control_flow(test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test agent: Start a new conversation\n",
    "\n",
    "######  start the imports\n",
    "\n",
    "import agent\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "graph = agent.graph\n",
    "create_config = agent.create_config\n",
    "objects_documentation = agent.objects_documentation\n",
    "sql_dialect = agent.sql_dialect\n",
    "\n",
    "# Import initialization components\n",
    "from src.init.initialization import (\n",
    "    llm, llm_fast, create_config, tracer,\n",
    "    objects_documentation, sql_dialect, connection_string\n",
    ")\n",
    "\n",
    "###### \n",
    "\n",
    "question = 'Which practice segment has the highest asset growth rate in the last 12 months?'\n",
    "messages_log = []\n",
    "\n",
    "initial_dict = {'objects_documentation':objects_documentation,\n",
    "     'sql_dialect': sql_dialect,\n",
    "     'messages_log': messages_log,\n",
    "     'intermediate_steps':[],\n",
    "     'analytical_intent': [],\n",
    "     'current_question':question,\n",
    "     'current_sql_queries': [],\n",
    "     'generate_answer_details': {},\n",
    "     'llm_answer': AIMessage(content=''),\n",
    "     'scenario': '',\n",
    "     'search_terms_output':{}\n",
    "     }\n",
    "     \n",
    "vector_store = None  # reset vector store\n",
    "config, thread_id = create_config('Run Agent',True)\n",
    "\n",
    "result = graph.invoke(initial_dict, config = config)\n",
    "display = f'''Analytical intent: {result['analytical_intent']}\\n\\nSQL query: {result['current_sql_queries']}\\n\\nGenerate Answer Details: {result['generate_answer_details']}\\n\\nAnswer: {result['llm_answer']}'''\n",
    "print(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the conversation with the graph\n",
    "question = 'first business line'\n",
    "\n",
    "initial_dict['current_question'] = question\n",
    "vector_store = None  # reset vector store\n",
    "config, _ = create_config('generate_answer', False, thread_id) # (re-use same thread)\n",
    "result = graph.invoke(initial_dict, config)\n",
    "display = f'''Analytical intent: {result['analytical_intent']}\\n\\nSQL query: {result['current_sql_queries']}\\n\\nGenerate Answer Details: {result['generate_answer_details']}\\n\\nAnswer: {result['llm_answer']}'''\n",
    "print(display)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a0fce415d2e9e6477a25b7ed122e649823ddf5df417fe3f69c00ade6d8eb08b"
  },
  "kernelspec": {
   "display_name": "Python 3.13.5 ('.venv-py313': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
