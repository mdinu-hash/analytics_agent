{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc147a51",
   "metadata": {},
   "source": [
    "### Import feedbacks.db file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test access to db file: import db tables into data frames and select by the column names\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import uuid\n",
    "\n",
    "engine = create_engine('sqlite:///feedbacks_db.db')\n",
    "inspector = inspect(engine)\n",
    "\n",
    "df_company = pd.read_sql_query('SELECT company_name,annual_revenue_usd FROM company', engine)\n",
    "df_feedback = pd.read_sql_query('SELECT feedback_id,feedback_date,product_id,product_company_name,feedback_text,\"feedback_rating\" FROM feedback', engine)\n",
    "df_products = pd.read_sql_query('SELECT product_id,product_name,product_brand,product_manufacturer,product_company_name,product_price,product_average_rating FROM products', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4008f31",
   "metadata": {},
   "source": [
    "### Instantiate chat model (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de278ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain, langgraph, langchain_openai, langsmith\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ['LANGSMITH_API_KEY'] = LANGSMITH_API_KEY\n",
    "os.environ['LANGSMITH_TRACING'] = \"true\"\n",
    "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "langsmith_project_name = \"db_agent_v1\"\n",
    "os.environ['LANGSMITH_PROJECT'] = langsmith_project_name\n",
    "\n",
    "# Set up LangSmith tracer manually\n",
    "tracer = LangChainTracer(project_name=langsmith_project_name)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o',temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fc53d",
   "metadata": {},
   "source": [
    "### Create config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b11c69a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def create_config(run_name: str, is_new_thread_id: bool = False, thread_id: str = None):\n",
    "    \"\"\"\n",
    "    Create a config dictionary for LCEL runnables.\n",
    "    Includes LangSmith run tracing and optional thread_id management.\n",
    "\n",
    "    Args:\n",
    "        run_name (str): Descriptive run name shown in LangSmith.\n",
    "        is_new_thread_id (bool): Whether to generate a new thread_id.\n",
    "        thread_id (str): Optionally provide an existing thread_id to reuse.\n",
    "\n",
    "    Returns:\n",
    "        dict: Config dictionary with callbacks, run_name, and thread_id.\n",
    "\n",
    "    Use it like so (example): \n",
    "        config, thread_id = create_config('create_sql_query_or_queries', True) (start a new thread)\n",
    "        config, _ = create_config('generate_answer', False, thread_id) (re-use same thread)\n",
    "    \"\"\"\n",
    "\n",
    "    time_now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    full_run_name = f\"{run_name} {time_now}\"\n",
    "    if is_new_thread_id or not thread_id:\n",
    "        thread_id = str(uuid.uuid4())\n",
    "\n",
    "    config={'callbacks': [tracer],\n",
    "            'run_name': run_name,\n",
    "            'configurable' : { 'thread_id':thread_id }\n",
    "            }\n",
    "\n",
    "    return config,thread_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57674e30",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97c018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "objects_documentation = \"\"\"\n",
    "  Table company: List of public companies. Granularity is company-name. Column (prefixed with table name):\n",
    "  company.company-name: the name of the public company.\n",
    "  company.annual_revenue_usd: revenue in last 12 months ($).\n",
    "\n",
    "  Table feedback: Feedbacks given by clients to products. Granularity is feedback. Key is feedback_id. Columns (prefixed with table name):\n",
    "  feedback.feedback_id: id of the feedback.\n",
    "  feedback.feedback_date: date of feedback.\n",
    "  feedback.product_id: id of the product the feedback was given for.\n",
    "  feedback.product_company_name: company owning the product.\n",
    "  feedback.feedback_text: the text of the feedback.\n",
    "  feedback.feedback_rating: rating of the feedback from 1 to 5, 5 being the highest score.\n",
    "\n",
    "  Table products: Shows product metadata. Granularity is product. Key is product_id. Columns (prefixed with table name):\n",
    "  products.product_id: id of the product.\n",
    "  products.product_name: name of the product.\n",
    "  products.product_brand: the brand under which the product was presented.\n",
    "  products.product_manufacturer: product manufacturer.\n",
    "  products.product_company_name: company owning the product.\n",
    "  products.product_price: price of the product at crawling time.\n",
    "  products.product_average-rating: average ratings across all feedbacks for the product, at crawling time.\n",
    "\n",
    "  Table company -> column company_name relates to table feedback -> column product_company_name\n",
    "  Table products -> column product_company_name relates to table feedback -> column product_company-name\n",
    "  Table feedback -> column product_id relates to table products -> column product_id\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977899ef",
   "metadata": {},
   "source": [
    "### Define state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d272a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the state of the graph, which includes user's question, AI's answer, query that has been created and its result;\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, RemoveMessage\n",
    "\n",
    "class State(TypedDict):\n",
    " messages_log: Sequence[BaseMessage]\n",
    " question: str\n",
    " sql_queries: list[dict]\n",
    " llm_answer: BaseMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27d37d",
   "metadata": {},
   "source": [
    "### Create sql query or queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a830ba6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# create a function that generates the sql query to be executed\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
    "\n",
    "class OutputAsAQuery(TypedDict):\n",
    "  \"\"\" generated sql query or sql queries if there are multiple \"\"\"\n",
    "  query: Annotated[list[str],\"clean sql query\"]\n",
    "\n",
    "def create_sql_query_or_queries(state:State):\n",
    "  \"\"\" creates a sql query based on the question \"\"\"\n",
    "\n",
    "  system_prompt = \"\"\"You are a sql expert and an expert data modeler.  \n",
    "\n",
    "  Your task is to create a sql script to answer the user's question. In the sql script, use only these tables and columns you have access to:\n",
    "  {objects_documentation}\n",
    "\n",
    "  User question:\n",
    "  {question}\n",
    "\n",
    "  Answer just with the resulting sql code.\n",
    "\n",
    "  IMPORTANT:\n",
    "    - Return only raw SQL strings in the list.\n",
    "    - DO NOT include comments (like \"-- Query 1\"), labels, or explanations.\n",
    "    - If only one SQL query is needed, just return a list with that one query.\n",
    "    - Do not generate more than 5 queries.\n",
    "\n",
    "  Example output:\n",
    "    [\n",
    "      \"SELECT COUNT(*) FROM feedback;\",\n",
    "      \"SELECT AVG(product_price) FROM products;\"\n",
    "    ]\n",
    "  \"\"\"\n",
    "\n",
    "  prompt = ChatPromptTemplate.from_messages(\n",
    "    ('system', system_prompt)\n",
    "  )\n",
    "\n",
    "  chain = prompt | llm.with_structured_output(OutputAsAQuery)\n",
    "\n",
    "  result = chain.invoke({'objects_documentation':objects_documentation, 'question': state['question']})\n",
    "  for q in result['query']:\n",
    "   state['sql_queries'].append( {'query': q,\n",
    "                                     'explanation': '', ## add it later\n",
    "                                     'result':'' ## add it later\n",
    "                                      } )\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188b45f9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# since gpt-4o allows a maximum completion limit (output context limit) of 4k tokens, I half it to get maximum context size, so 2k. Assuming the entire context is not just the data,\n",
    "# I divide this number by 5 and arrive at a limit of 400 tokens for the result of the sql query.\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "maximum_nr_tokens_sql_query = 200\n",
    "\n",
    "# create a function that counts the tokens from a string\n",
    "def count_tokens(string:str):\n",
    " \"\"\" returns the number of tokens in a text string \"\"\"\n",
    " encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    " num_tokens = len(encoding.encode(string))\n",
    " return num_tokens\n",
    "\n",
    "# create a function that compares the tokens from the sql query result with the maximum token limit, and returns true if the context limit has been exceeded, false otherwise.\n",
    "def check_if_exceed_maximum_context_limit(sql_query_result):\n",
    " \"\"\" compares the tokens from the sql query result with the maximum token limit, and returns true if the context limit has been exceeded, false otherwise \"\"\"\n",
    " tokens_sql_query_result = count_tokens(sql_query_result)\n",
    " if tokens_sql_query_result > maximum_nr_tokens_sql_query:\n",
    "  return True\n",
    " else:\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0992424",
   "metadata": {},
   "source": [
    "### Create sql query explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2613a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# create a function that creates an explanation of a sql query\n",
    "\n",
    "def create_sql_query_explanation(sql_query:str):\n",
    " \"\"\" creates a concise explanation of the sql query \"\"\"\n",
    "\n",
    " system_prompt = \"\"\"\n",
    " As a data expert, you are provided with this sql query:\n",
    " {sql_query}\n",
    "\n",
    " Create a brief explanation of this query in simple terms by taking into account these factors, if exist:\n",
    " - Pay attention to the nuances of the query: the filters, aggregations, groupings, etc.\n",
    " - Take into account underlying assumptions.\n",
    " - Query limitations.\n",
    " Keep it short.\n",
    " \"\"\"\n",
    "\n",
    " prompt = ChatPromptTemplate.from_messages(\n",
    "     ('system',system_prompt)\n",
    " )\n",
    "\n",
    " chain = prompt | llm\n",
    " sql_query_explanation = chain.invoke({'sql_query':sql_query}).content\n",
    " return sql_query_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7489351",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class OutputAsASingleQuery(TypedDict):\n",
    "  \"\"\" generated sql query \"\"\"\n",
    "  query: Annotated[str,...,\"the generated sql query\"]\n",
    "\n",
    "def refine_sql_query(question: str, sql_query: str, maximum_nr_tokens_sql_query: int):\n",
    " \"\"\" refines the sql query so that its output tokens do not exceed the maximum context limit \"\"\"\n",
    "\n",
    " system_prompt = \"\"\"\n",
    " You are a sql expert an an expert data modeler.\n",
    "\n",
    " You are tying to answer the following question from the user:\n",
    " {question}\n",
    "\n",
    " The following sql query produces an output that exceeds {maximum_nr_tokens_sql_query} tokens:\n",
    " {sql_query}\n",
    "\n",
    " Please optimize this query so that its output stays within the token limit while still providing as much insight as possible to answer the question.\n",
    " Prefer using WHERE or LIMIT clauses to reduce the size of the result.\n",
    " \"\"\"\n",
    " \n",
    " prompt = ChatPromptTemplate.from_messages(\n",
    "   ('system',system_prompt)\n",
    " )\n",
    "\n",
    " chain = (prompt\n",
    "         | llm.with_structured_output(OutputAsASingleQuery)\n",
    " )\n",
    "\n",
    " sql_query = chain.invoke({'question': question,\n",
    "               'sql_query':sql_query,\n",
    "               'maximum_nr_tokens_sql_query':maximum_nr_tokens_sql_query}\n",
    "               )\n",
    " return sql_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1c989",
   "metadata": {},
   "source": [
    "### Execute sql query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f766366d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# the function checks if the query output exceeds context window limit and if yes, send the query for refinement\n",
    "\n",
    "from langchain_community.tools import QuerySQLDataBaseTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from typing import Iterator\n",
    "\n",
    "db = SQLDatabase(engine)\n",
    "\n",
    "def execute_sql_query(state:State):\n",
    "  \"\"\" executes the sql query and retrieve the result \"\"\"\n",
    "\n",
    "  for query_index, q in enumerate(state['sql_queries']):\n",
    "     \n",
    "    sql_query = q['query'] \n",
    "    print(f\"🚀 Executing query {query_index+1}/{len(state['sql_queries'])}...\")\n",
    "    # refine the query 3 times if necessary.\n",
    "    for i in range(3):\n",
    "\n",
    "      sql_query_result = QuerySQLDataBaseTool(db=db).invoke(sql_query)\n",
    "\n",
    "      # if the sql query does not exceed output context window return its result\n",
    "      if not check_if_exceed_maximum_context_limit(sql_query_result):\n",
    "\n",
    "       sql_query_explanation = create_sql_query_explanation(sql_query)\n",
    "       state['sql_queries'][query_index]['result'] = sql_query_result\n",
    "       state['sql_queries'][query_index]['explanation'] = sql_query_explanation\n",
    "       state['sql_queries'][query_index]['query'] = sql_query\n",
    "       break\n",
    "\n",
    "      # if the sql query exceeds output context window and there is more room for iterations, refine the query\n",
    "      else:\n",
    "        print(f\"🔧 Refining query {query_index+1}/{len(state['sql_queries'])} as its output its too large...\")\n",
    "        sql_query = refine_sql_query(state['question'],sql_query,maximum_nr_tokens_sql_query)['query']\n",
    "\n",
    "      # if there is no more room for sql query iterations and the result still exceeds context window, throw a message\n",
    "\n",
    "    else:\n",
    "      print(f\"⚠️ Query result too large after 3 refinements.\")\n",
    "      state['sql_queries'][query_index]['result'] = 'Query result too large after 3 refinements.'\n",
    "      state['sql_queries'][query_index]['explanation'] = \"Refinement failed.\"\n",
    "      \n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea927d0a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "### Extract metadata from sql query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231961b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "from sqlglot import parse_one, exp\n",
    "\n",
    "def extract_metadata_from_sql_query(sql_query):\n",
    " ast = parse_one(sql_query)\n",
    "\n",
    " sql_query_metadata = {\n",
    "    \"tables\": [],\n",
    "    \"filters\": [],\n",
    "    \"aggregations\": [],\n",
    "    \"groupings\": []\n",
    " }\n",
    "\n",
    " # extract tables\n",
    " table_generator = ast.find_all(sqlglot.expressions.Table)\n",
    " for items in table_generator:\n",
    "    sql_query_metadata['tables'].append(items.sql())\n",
    " # remove dups\n",
    " sql_query_metadata['tables'] = list(dict.fromkeys(sql_query_metadata['tables']))\n",
    "\n",
    " # extract filters\n",
    " where_conditions = ast.find_all(sqlglot.expressions.Where)\n",
    " for item in where_conditions:\n",
    "  sql_query_metadata['filters'].append(item.this.sql())\n",
    "  # remove dups\n",
    " sql_query_metadata['filters'] = list(dict.fromkeys(sql_query_metadata['filters']))\n",
    "\n",
    " # extract aggregate functions\n",
    " funcs = ast.find_all(sqlglot.expressions.AggFunc)\n",
    " for item in funcs:\n",
    "  sql_query_metadata['aggregations'].append(item.sql())\n",
    " # remove dups\n",
    " sql_query_metadata['aggregations'] = list(dict.fromkeys(sql_query_metadata['aggregations']))\n",
    "\n",
    " # extract groupings\n",
    " groupings = ast.find_all(sqlglot.expressions.Group)\n",
    " for item in groupings:\n",
    "  groupings_flattened = item.flatten()\n",
    "  for item in groupings_flattened:\n",
    "    sql_query_metadata['groupings'].append(item.sql())\n",
    " # remove dups\n",
    " sql_query_metadata['groupings'] = list(dict.fromkeys(sql_query_metadata['groupings']))\n",
    "\n",
    " return sql_query_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3a2ff8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_explanation(sql_queries: list[dict]):\n",
    " \"\"\" based on the sql query metadata that was parsed, it creates a natural language message describing filters and transformations used by the query\"\"\"\n",
    "\n",
    " tables = []\n",
    " filters = []\n",
    " aggregations = []\n",
    " groupings = []\n",
    "\n",
    " for query_index,q in enumerate(sql_queries):\n",
    " # get sql query metadata\n",
    "  sql_query = q['query']\n",
    "  sql_query_metadata = extract_metadata_from_sql_query(sql_query)\n",
    "\n",
    "  if sql_query_metadata['tables']:\n",
    "   tables.extend(sql_query_metadata['tables'])\n",
    "   tables = list(dict.fromkeys(tables))\n",
    "\n",
    "  if sql_query_metadata['filters']:\n",
    "   filters.extend(sql_query_metadata['filters'])\n",
    "   filters = list(dict.fromkeys(filters))\n",
    "\n",
    "  if sql_query_metadata['aggregations']:\n",
    "   aggregations.extend(sql_query_metadata['aggregations'])\n",
    "   aggregations = list(dict.fromkeys(aggregations))\n",
    "\n",
    "  if sql_query_metadata['groupings']:\n",
    "   groupings.extend(sql_query_metadata['groupings'])\n",
    "   groupings = list(dict.fromkeys(groupings))\n",
    "\n",
    " # wrapping it all together\n",
    " sql_query_explanation = \"I analyzed data based on the following filters and transformations:\"\n",
    "\n",
    " if tables:\n",
    "  tables = f\"🧊 Tables: • {' • '.join(tables)}\"\n",
    "  sql_query_explanation = sql_query_explanation + \"\\n\\n\" + tables\n",
    "\n",
    " if filters:\n",
    "  filters = f\"🔍 Filters: • {' • '.join(filters)}\"\n",
    "  sql_query_explanation = sql_query_explanation + \"\\n\\n\" + filters\n",
    "\n",
    " if aggregations:\n",
    "  aggregations = f\"🧮 Aggregations: • {' • '.join(aggregations)}\"\n",
    "  sql_query_explanation = sql_query_explanation + \"\\n\\n\" + aggregations\n",
    "\n",
    " if groupings:\n",
    "  groupings = f\"📦 Groupings: • {' • '.join(groupings)}\"\n",
    "  sql_query_explanation = sql_query_explanation + \"\\n\\n\" + groupings\n",
    "\n",
    " return sql_query_explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952f76b",
   "metadata": {},
   "source": [
    "### Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65eaed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sql_queries_for_prompt (sql_queries : list[dict]) -> str:\n",
    "    # expects a dictionary with a structure like query, explanation, result\n",
    "    \n",
    "    formatted_queries = []\n",
    "    for query_index,q in enumerate(sql_queries):\n",
    "        formatted_queries.append(f\"Query {query_index+1} explanation:\\n {q['explanation']}\\n\\n Query {query_index+1} result:\\n {q['result']}\")\n",
    "    return \"\\n\\n\".join(formatted_queries)\n",
    "\n",
    "# print(format_sql_queries_for_prompt(test_state['sql_queries']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17735fb5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## create a function that generates the agent answer based on sql query result\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "def generate_answer(state:State):\n",
    "  \"\"\" generates the AI answer taking into consideration the explanation and the result of the sql query that was executed \"\"\"\n",
    "\n",
    "  system_prompt = \"\"\" You are a decision support consultant helping users become more data-driven.\n",
    "     Your task is to answer the user question using the result of the sql queries:\n",
    "\n",
    " - The sql queries were created for the purpose of answering the user question.\n",
    " - The query explanations are short descriptions of the query making you aware of its limitations and underlying assumptions.\n",
    "\n",
    " User question:\n",
    " {question}\n",
    "\n",
    " Context from SQL queries:\n",
    " {context_sql_queries}\n",
    "\n",
    " Take into account the insights from this explanation in your answer.\n",
    " Answer in simple terms, conversational, non-technical language. Be concise.\n",
    " \"\"\"\n",
    "\n",
    "  prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system',system_prompt),\n",
    "    MessagesPlaceholder(\"messages_log\")          \n",
    "  ] )\n",
    "\n",
    "  llm_answer_chain = prompt | llm \n",
    "  final_answer_chain = { 'llm_answer': llm_answer_chain, 'input_state': RunnablePassthrough() } | RunnableLambda (lambda x: { 'ai_message': AIMessage( content = f\"{x['llm_answer'].content}\\n\\n{create_explanation(x['input_state']['sql_queries'])}\", \n",
    "                                                                                                                                                        response_metadata = x['llm_answer'].response_metadata  ) } ) \n",
    "\n",
    "  result = final_answer_chain.invoke({ 'messages_log':state['messages_log'],\n",
    "               'question':state['question'],\n",
    "               'context_sql_queries': format_sql_queries_for_prompt(state['sql_queries']),\n",
    "              'sql_queries': state['sql_queries'] })\n",
    "  \n",
    "  ai_msg = result['ai_message']\n",
    "\n",
    "  explanation_token_count = llm.get_num_tokens(create_explanation(state['sql_queries']))\n",
    "  ai_msg.response_metadata['token_usage']['total_tokens'] += explanation_token_count\n",
    "\n",
    "  state['llm_answer'] = ai_msg\n",
    "  state['messages_log'].append(HumanMessage(state['question']))\n",
    "  state['messages_log'].append(ai_msg)\n",
    "\n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68cae6",
   "metadata": {},
   "source": [
    "### Manage memory and chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f942bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_memory_chat_history(state:State):\n",
    "    \"\"\" Manages the chat history so that it does not become too large in terms of output tokens.\n",
    "    Specifically, it checks if the chat history is larger than 1000 tokens. If yes, keep just the last 4 pairs of human prompts and AI responses, and summarize the older messages.\n",
    "    Additionally, check if the logs of sql queries is larger than 20 entries. If yes, delete the older records. \"\"\"           \n",
    "\n",
    "    tokens_chat_history = state['messages_log'][-1].response_metadata.get('token_usage', {}).get('total_tokens', 0) if state['messages_log'] else 0\n",
    "    \n",
    "\n",
    "    if tokens_chat_history >= 1000 and len(state['messages_log']) > 4:\n",
    "        message_history_to_summarize = [msg.content for msg in state['messages_log'][:-4]]\n",
    "        prompt = ChatPromptTemplate.from_messages( [('user', 'Distill the below chat messages into a single summary paragraph.The summary paragraph should have maximum 400 tokens.Include as many specific details as you can.Chat messages:{message_history_to_summarize}') ])\n",
    "        runnable = prompt | llm\n",
    "        chat_history_summary = runnable.invoke({'message_history_to_summarize':message_history_to_summarize})\n",
    "        last_4_messages = state['messages_log'][-4:]\n",
    "        state['messages_log'] = [chat_history_summary] +[*last_4_messages]\n",
    "    else:\n",
    "        state['messages_log'] = state['messages_log']\n",
    "\n",
    "    # Truncate SQL logs to the most recent 20\n",
    "    if len(state['sql_queries']) > 20:\n",
    "        state['sql_queries']= state['sql_queries'][-20:]    \n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7ce8d",
   "metadata": {},
   "source": [
    "### Assemble graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23aa439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble graph\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "graph= StateGraph(State)\n",
    "graph.add_node(\"create_sql_query_or_queries\",create_sql_query_or_queries)\n",
    "graph.add_node(\"execute_sql_query\",execute_sql_query)\n",
    "graph.add_node(\"generate_answer\",generate_answer)\n",
    "graph.add_node(\"manage_memory_chat_history\",manage_memory_chat_history)\n",
    "\n",
    "graph.add_edge(START,\"create_sql_query_or_queries\")\n",
    "graph.add_edge(\"create_sql_query_or_queries\",\"execute_sql_query\")\n",
    "graph.add_edge(\"execute_sql_query\",\"generate_answer\")\n",
    "graph.add_edge(\"generate_answer\",\"manage_memory_chat_history\")\n",
    "graph.add_edge(\"manage_memory_chat_history\",END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcd929",
   "metadata": {},
   "source": [
    "\n",
    "### test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new conversation\n",
    "\n",
    "question = 'How many companies are there?'\n",
    "messages_log = []\n",
    "\n",
    "initial_dict = {'objects_documentation':objects_documentation,\n",
    "     'messages_log': messages_log,\n",
    "     'question':question,\n",
    "     'sql_queries': [],\n",
    "     'llm_answer': []\n",
    "     }\n",
    "\n",
    "config, thread_id = create_config('Run Agent',True)\n",
    "if __name__ == \"__main__\":\n",
    " for step in graph.stream(initial_dict, config = config, stream_mode=\"updates\"):\n",
    "   step_name, output = list(step.items())[0]\n",
    "   if step_name == 'create_sql_query_or_queries':\n",
    "    print(f\"✅ SQL queries created:{len(output['sql_queries'])}\")\n",
    "   elif step_name == 'execute_sql_query':\n",
    "    print(\"⚙️ Analysing results...\")\n",
    "   elif step_name == 'generate_answer':\n",
    "    print(\"\\n📣 Final Answer:\\n\")\n",
    "    print(output['llm_answer'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "\n",
    "initial_dict['question'] = 'follow up question' \n",
    "\n",
    "\n",
    "config, _ = create_config('Run Agent',False,thread_id)\n",
    "if __name__ == \"__main__\":\n",
    " for step in graph.stream(initial_dict, config = config, stream_mode=\"updates\"):\n",
    "   step_name, output = list(step.items())[0]\n",
    "   if step_name == 'create_sql_query_or_queries':\n",
    "    print(f\"✅ SQL queries created:{len(output['sql_queries'])}\")\n",
    "   elif step_name == 'execute_sql_query':\n",
    "    print(\"⚙️ Analysing results...\")\n",
    "   elif step_name == 'generate_answer':\n",
    "    print(\"\\n📣 Final Answer:\\n\")\n",
    "    print(output['llm_answer'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ffa2c",
   "metadata": {},
   "source": [
    "### Testing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da775371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = 'How many companies are there?'\n",
    "\n",
    "# test_state = {'messages_log':[],\n",
    "#  'question':question,\n",
    "#  'sql_queries': [],\n",
    "#  'llm_answer': []\n",
    "#  }\n",
    "# create_sql_query_or_queries(test_state)\n",
    "# execute_sql_query(test_state)\n",
    "# generate_answer(test_state)\n",
    "# manage_memory_chat_history(test_state)\n",
    "# #test_state "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "951c4e1ccc8e7dc066b7b3456b4d29f8a6c8c8949bd81a565897b5da2568416e"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
