{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a56df5",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc147a51",
   "metadata": {},
   "source": [
    "### Import feedbacks.db file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5b824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test access to db file: import db tables into data frames and select by the column names\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import uuid\n",
    "\n",
    "engine = create_engine('sqlite:///feedbacks_db.db')\n",
    "inspector = inspect(engine)\n",
    "\n",
    "df_company = pd.read_sql_query('SELECT company_name,annual_revenue_usd FROM company', engine)\n",
    "df_feedback = pd.read_sql_query('SELECT feedback_id,feedback_date,product_id,product_company_name,feedback_text,\"feedback_rating\" FROM feedback', engine)\n",
    "df_products = pd.read_sql_query('SELECT product_id,product_name,product_brand,product_manufacturer,product_company_name,product_price,product_average_rating FROM products', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4008f31",
   "metadata": {},
   "source": [
    "### Instantiate chat model (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de278ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain, langgraph, langchain_openai, langsmith\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ['LANGSMITH_API_KEY'] = LANGSMITH_API_KEY\n",
    "os.environ['LANGSMITH_TRACING'] = \"true\"\n",
    "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "langsmith_project_name = \"db_agent_v1\"\n",
    "os.environ['LANGSMITH_PROJECT'] = langsmith_project_name\n",
    "\n",
    "# Set up LangSmith tracer manually\n",
    "tracer = LangChainTracer(project_name=langsmith_project_name)\n",
    "\n",
    "from langchain_openai import ChatOpenAI \n",
    "llm = ChatOpenAI(model='gpt-4.1',temperature=0) # Smart & expensive\n",
    "llm_fast = ChatOpenAI(model='gpt-4o',temperature=0) # Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fc53d",
   "metadata": {},
   "source": [
    "### Create config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b11c69a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def create_config(run_name: str, is_new_thread_id: bool = False, thread_id: str = None):\n",
    "    \"\"\"\n",
    "    Create a config dictionary for LCEL runnables.\n",
    "    Includes LangSmith run tracing and optional thread_id management.\n",
    "\n",
    "    Args:\n",
    "        run_name (str): Descriptive run name shown in LangSmith.\n",
    "        is_new_thread_id (bool): Whether to generate a new thread_id.\n",
    "        thread_id (str): Optionally provide an existing thread_id to reuse.\n",
    "\n",
    "    Returns:\n",
    "        dict: Config dictionary with callbacks, run_name, and thread_id.\n",
    "\n",
    "    Use it like so (example): \n",
    "        config, thread_id = create_config('create_sql_query_or_queries', True) (start a new thread)\n",
    "        config, _ = create_config('generate_answer', False, thread_id) (re-use same thread)\n",
    "    \"\"\"\n",
    "\n",
    "    time_now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    full_run_name = f\"{run_name} {time_now}\"\n",
    "    if is_new_thread_id or not thread_id:\n",
    "        thread_id = str(uuid.uuid4())\n",
    "\n",
    "    config={'callbacks': [tracer],\n",
    "            'run_name': full_run_name,\n",
    "            'configurable' : { 'thread_id':thread_id }\n",
    "            }\n",
    "\n",
    "    return config,thread_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebaf8f1",
   "metadata": {},
   "source": [
    "### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7348f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = None\n",
    "\n",
    "objects_documentation = '''Table company: List of public companies. Granularity is company-name. Column (prefixed with table name):\n",
    "     company.company-name: the name of the public company.\n",
    "     company.annual_revenue_usd: revenue in last 12 months ($).\n",
    "\n",
    "     Table feedback: Feedbacks given by clients to products. Granularity is feedback. Key is feedback_id. Columns (prefixed with table name):\n",
    "     feedback.feedback_id: id of the feedback.\n",
    "     feedback.feedback_date: date of feedback.\n",
    "     feedback.product_id: id of the product the feedback was given for.\n",
    "     feedback.product_company_name: company owning the product.\n",
    "     feedback.feedback_text: the text of the feedback.\n",
    "     feedback.feedback_rating: rating of the feedback from 1 to 5, 5 being the highest score.\n",
    "\n",
    "     Table products: Shows product metadata. Granularity is product. Key is product_id. Columns (prefixed with table name):\n",
    "     products.product_id: id of the product.\n",
    "     products.product_name: name of the product.\n",
    "     products.product_brand: the brand under which the product was presented.\n",
    "     products.product_manufacturer: product manufacturer.\n",
    "     products.product_company_name: company owning the product.\n",
    "     products.product_price: price of the product at crawling time.\n",
    "     products.product_average_rating: average ratings across all feedbacks for the product, at crawling time.\n",
    "\n",
    "     Table company -> column company_name relates to table feedback -> column product_company_name\n",
    "     Table products -> column product_company_name relates to table feedback -> column product_company-name\n",
    "     Table feedback -> column product_id relates to table products -> column product_id'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977899ef",
   "metadata": {},
   "source": [
    "### Define state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d272a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the state of the graph, which includes user's question, AI's answer, query that has been created and its result;\n",
    "from typing_extensions import TypedDict, Annotated, Literal, Union\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, RemoveMessage\n",
    "from langchain_core.agents import AgentAction\n",
    "import operator\n",
    "\n",
    "class State(TypedDict):\n",
    " objects_documentation: str\n",
    " messages_log: Sequence[BaseMessage]\n",
    " intermediate_steps: list[AgentAction]\n",
    " analytical_intent: list[str]\n",
    " current_question: str\n",
    " current_sql_queries: list[dict]\n",
    " generate_answer_details: dict\n",
    " llm_answer: BaseMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496f5c4",
   "metadata": {},
   "source": [
    "## Extract analytical intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4788c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "def extract_msg_content_from_history(messages_log:list):\n",
    " ''' from a list of base messages, extract just the content '''\n",
    " content = []\n",
    " for msg in messages_log:\n",
    "     content.append(msg.content)\n",
    " return \"\\n\".join(content)\n",
    "\n",
    "class ClearOrAmbiguous(TypedDict):\n",
    "  ''' conclusion about the analytical intent extraction process '''\n",
    "  scenario: Annotated[Literal[\"Analytical Intent Extracted\", \"Analytical Intent Ambiguous\"],\"conclusion about the analytical intent extraction process\"] \n",
    "\n",
    "class AnalyticalIntents(TypedDict):\n",
    "  ''' list of analytical intents '''\n",
    "  analytical_intent: Annotated[Union[list[str], None] ,\"natural language descriptions to capture the analytical intents\"]                                      \n",
    "\n",
    "@tool\n",
    "def extract_analytical_intent(state:State):\n",
    "  ''' generates a natural language description to capture the analytical intent and refine the user ask ''' \n",
    "  \n",
    "  sys_prompt_clear_or_ambiguous = \"\"\"Decide whether the user question is clear or ambigous based on this specific database schema:\n",
    "  {objects_documentation}.\n",
    "\n",
    "  Conversation history:\n",
    "  \"{messages_log}\".\n",
    "\n",
    "  User question:\n",
    "  \"{question}\".\n",
    "\n",
    "  *** The question is clear if ***\n",
    "  - It has a single, obvious analytical approach in terms of grouping, filtering, aggregations using available columns and relationships or past conversations.\n",
    "\n",
    "  - The column and metric naming in the schema clearly points to one dominant method of interpretation. \n",
    "    Example: \"what is the top client?\" is clear in a database schema that contains just 1 single metric that can answer the question (ex: sales_amount). \n",
    "  \n",
    "  - The question is exploratory or open-ended.\n",
    "    Example: \"What can you tell me about the dataset?\".\n",
    "  \n",
    "  *** The question is ambigous if ***\n",
    "  - The question could be answered from different analytical intents that use different metrics, grouping, filtering or aggregations.\n",
    "    Example: Use pre-aggregated metrics vs metrics computed from aggregations across detailed tables.\n",
    "\n",
    "  - It can be answered by different metrics or metric definitions.\n",
    "    Example: \"What is the top client?\" is ambigous in a database schema that contains multiple metrics that can answer the question (highest value of sales / highest number of sales). \n",
    "\n",
    "  Response format:\n",
    "  If clear -> \"Analytical Intent Extracted\".\n",
    "  If ambigous -> \"Analytical Intent Ambiguous\". \n",
    "  \"\"\"\n",
    "\n",
    "  sys_prompt_clear = \"\"\"Refine technically the user ask for a sql developer with access to the following database schema:\n",
    "  {objects_documentation}.\n",
    "\n",
    "  Conversation history:\n",
    "  \"{messages_log}\".\n",
    "\n",
    "  Last user prompt:\n",
    "  \"{question}\".  \n",
    "\n",
    "  Create 1 analytical intent to answer the user ask. Here is how:\n",
    "\n",
    "      - The analytical intent will be used to create a single sql query.\n",
    "      - Write it in 1 sentence.\n",
    "      - Mention just the column names, tables names, grouping levels, aggregation functions (preffered if it doesn't restrict insights) and filters from the database schema.  \n",
    "      - If the user ask is exploratory (ex: \"What can you tell me about the dataset?\"), create 3-5 analytical intents.    \n",
    "      - If the user ask is non-exploratory, create only one analytical intent.\n",
    "  \"\"\"\n",
    "\n",
    "  sys_prompt_ambiguous = \"\"\"\n",
    "  Conversation history:\n",
    "  \"{messages_log}\".\n",
    "\n",
    "  Last user prompt:\n",
    "  \"{question}\". \n",
    "\n",
    "  The last user question is ambiguous from the analytical point of view, because it can be answered using different analytical intents that can be interpreted in multiple ways leading to different results.\n",
    "  \n",
    "  That is, there are different sql queries with different metadata (object names/filters/aggregations) that can answer the question.\n",
    "\n",
    "  Your task is to create all analytical intents that can possibly answer the user question using the following database schema:  \n",
    "  {objects_documentation}.             \n",
    "\n",
    "  Important considerations about creating analytical intents:\n",
    "      - Each analytical intent is for creating one single sql query.\n",
    "      - Write each analytical intent using 1 sentence.\n",
    "      - Mention specific column names, tables names as well as aggregation functions (preffered if it doesn't restrict insights) and filters from the database schema.  \n",
    "      - Mention only the useful info for creating sql queries.   \n",
    "      - Do not include redundant intents. \n",
    "\n",
    "  Create one analytical intent for every possible pattern from the checklist that can answer the user quesion:  \n",
    "\n",
    "  ** Pattern Checklist **\n",
    "      1. filter on same table.\n",
    "        Example: select product_id from product table where avg_sales = 5.\n",
    "\n",
    "      2. Retrieve records from table A based on filter criteria from table B (assuming tables A and B are related).\n",
    "        Example: count of product_id from product table where unit_sale from sales table = 12.\n",
    "\n",
    "      3. filter records from table A based on calculated aggregations (AVG, SUM, COUNT) from table B (assuming tables A and B are related).\n",
    "        Example: count products from products table where AVG(amount) from sales table grouped by product > 100.     \n",
    "  \"\"\"\n",
    "  sys_prompt_notes = \"\"\"\n",
    "  Conversation history:\n",
    "  \"{messages_log}\".\n",
    "\n",
    "  Last user prompt:\n",
    "  \"{question}\". \n",
    "  \n",
    "  The last user question is ambiguous from the analytical point of view, because it can be answered using different analytical intents that can be interpreted in multiple ways leading to different results.\n",
    "  That is, there are different sql queries with different metadata (object names/filters/aggregations) that can answer the question.\n",
    "  The sql queries can only pull data from this database schema:\n",
    "  {objects_documentation}.\n",
    "\n",
    "  The different analytical intents that make the question ambiguous are the following:\n",
    "  {analytical_intents}.         \n",
    "  \n",
    "  Your task is to create an explanation of what makes the question unclear and show the alternatives.\n",
    "  Just acknowledge why is ambiguous and mention the alternatives, nothing more.\n",
    "  Be short, concise, explain in simple, non-technical language.\n",
    "  \"\"\"  \n",
    "\n",
    "  prompt_clear_or_ambiguous = ChatPromptTemplate.from_messages([('system', sys_prompt_clear_or_ambiguous)])\n",
    "  chain_1= prompt_clear_or_ambiguous | llm.with_structured_output(ClearOrAmbiguous)  \n",
    "\n",
    "  prompt_clear = ChatPromptTemplate.from_messages([('system', sys_prompt_clear)])\n",
    "  chain_2= prompt_clear | llm.with_structured_output(AnalyticalIntents)\n",
    "\n",
    "  prompt_ambiguous = ChatPromptTemplate.from_messages([('system', sys_prompt_ambiguous)])\n",
    "  chain_3= prompt_ambiguous | llm.with_structured_output(AnalyticalIntents)\n",
    "\n",
    "  prompt_notes = ChatPromptTemplate.from_messages([('system', sys_prompt_notes)])\n",
    "  chain_4= prompt_notes | llm_fast\n",
    "\n",
    "  # Prepare common input data\n",
    "  input_data = {\n",
    "        'objects_documentation': state['objects_documentation'], \n",
    "        'question': state['current_question'], \n",
    "        'messages_log': extract_msg_content_from_history(state['messages_log'])\n",
    "   }\n",
    "\n",
    "  # determine if clear or ambiguous\n",
    "  result_1 = chain_1.invoke(input_data)\n",
    "\n",
    "  # Based on result, invoke appropriate chain\n",
    "  if result_1['scenario'] == \"Analytical Intent Extracted\":\n",
    "        # create analytical intents\n",
    "        result_2 = chain_2.invoke(input_data)\n",
    "        # next tool to call \n",
    "        tool_name = 'retrieve_insights' \n",
    "        output = {\n",
    "            'scenario': result_1['scenario'],\n",
    "            'analytical_intent': result_2['analytical_intent'],\n",
    "            'notes': None\n",
    "        }\n",
    "  elif result_1['scenario'] == \"Analytical Intent Ambiguous\":\n",
    "         # create analytical intents\n",
    "         result_3 = chain_3.invoke(input_data)\n",
    "         input_data.update({'analytical_intents':result_3['analytical_intent']})\n",
    "         result_4 = chain_4.invoke(input_data)\n",
    "         # next tool to call \n",
    "         tool_name = 'generate_answer'\n",
    "         output = {\n",
    "            'scenario': result_1['scenario'], \n",
    "            'analytical_intent': result_3['analytical_intent'],\n",
    "            'notes': result_4.content }\n",
    "\n",
    "  # update the state \n",
    "  state['generate_answer_details'].update({'scenario':output['scenario'],\n",
    "                                           'notes':output['notes']})\n",
    "  state['analytical_intent'] = output['analytical_intent']\n",
    "  \n",
    "  # control flow\n",
    "  action = AgentAction(tool='extract_analytical_intent', tool_input='',log='tool ran successfully')\n",
    "  state['intermediate_steps'].append(action)\n",
    "  state['intermediate_steps'].append(AgentAction(tool=tool_name, tool_input='',log=''))    \n",
    "  \n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27d37d",
   "metadata": {},
   "source": [
    "## Create sql query or queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a830ba6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class OutputAsAQuery(TypedDict):\n",
    "  \"\"\" generated sql query or sql queries if there are multiple \"\"\"\n",
    "  query: Annotated[list[str],\"clean sql query\"]\n",
    "\n",
    "@tool\n",
    "def create_sql_query_or_queries(state:State):\n",
    "  \"\"\" creates sql query/queries to anwser a question based on documentation of tables and columns available \"\"\"\n",
    "\n",
    "  system_prompt = \"\"\"You are a sql expert and an expert data modeler.  \n",
    "\n",
    "  Your task is to create sql scripts in snowflake dialect to answer the analytical intent(s). In each sql script, use only these tables and columns you have access to:\n",
    "  {objects_documentation}\n",
    "\n",
    "  Analytical intent(s):\n",
    "  {analytical_intent}\n",
    "\n",
    "  Answer just with the resulting sql code(s).\n",
    "\n",
    "  IMPORTANT:\n",
    "    - Return one sql string for every analytical intent.\n",
    "    - Return only raw SQL strings in the list.\n",
    "    - DO NOT include comments (like \"-- Query 1\"), labels, or explanations.\n",
    "    - If only one SQL query is needed, just return a list with that one query.\n",
    "    - GROUP BY expressions must match the non-aggregated SELECT expressions.\n",
    "    - Ensure that any expression used in ORDER BY also appears in the SELECT clause.\n",
    "    - If you filter by specific text values, use trim and lowercase (ex: \"where trim(lower(column_name)) = trim(lower(\"ValueTofilterBy\")) \").  \n",
    "\n",
    "  Example output:\n",
    "    [\n",
    "      \"SELECT COUNT(*) FROM feedback;\",\n",
    "      \"SELECT AVG(product_price) FROM products;\"\n",
    "    ]\n",
    "  \"\"\"\n",
    "\n",
    "  prompt = ChatPromptTemplate.from_messages([('system', system_prompt)])\n",
    "\n",
    "  chain = prompt | llm.with_structured_output(OutputAsAQuery)\n",
    "\n",
    "  result = chain.invoke({'objects_documentation':state['objects_documentation'], 'analytical_intent': state['analytical_intent']})\n",
    "  for q in result['query']:\n",
    "   state['current_sql_queries'].append( {'query': q,\n",
    "                                     'explanation': '', ## add it later\n",
    "                                     'result':'', ## add it later\n",
    "                                     'insight': '', ## add it later\n",
    "                                     'metadata':'' ## add it later\n",
    "                                      } )\n",
    "  \n",
    "  print(f\"✅ SQL queries created:{len(state['current_sql_queries'])}\")\n",
    "  \n",
    "  # control flow\n",
    "  action = AgentAction(tool='create_sql_query_or_queries', tool_input='',log='tool ran successfully')\n",
    "  state['intermediate_steps'].append(action)  \n",
    "  return state\n",
    "\n",
    "# since gpt-4o allows a maximum completion limit (output context limit) of 4k tokens, I half it to get maximum context size, so 2k. Assuming the entire context is not just the data,\n",
    "# I divide this number by 5 and arrive at a limit of 400 tokens for the result of the sql query.\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "maximum_nr_tokens_sql_query = 500\n",
    "\n",
    "# create a function that counts the tokens from a string\n",
    "def count_tokens(string:str):\n",
    " \"\"\" returns the number of tokens in a text string \"\"\"\n",
    " encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    " num_tokens = len(encoding.encode(string))\n",
    " return num_tokens\n",
    "\n",
    "# create a function that compares the tokens from the sql query result with the maximum token limit, and returns true if the context limit has been exceeded, false otherwise.\n",
    "def check_if_exceed_maximum_context_limit(sql_query_result):\n",
    " \"\"\" compares the tokens from the sql query result with the maximum token limit, and returns true if the context limit has been exceeded, false otherwise \"\"\"\n",
    " tokens_sql_query_result = count_tokens(sql_query_result)\n",
    " if tokens_sql_query_result > maximum_nr_tokens_sql_query:\n",
    "  return True\n",
    " else:\n",
    "  return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0992424",
   "metadata": {},
   "source": [
    "### Create query analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d949731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAnalysis(TypedDict):\n",
    "    ''' complete analysis of a sql query, including its explanation, limitation and insight '''\n",
    "    explanation: str\n",
    "    limitation: str\n",
    "    insight: str\n",
    "\n",
    "def create_query_analysis(sql_query:str, sql_query_result:str):\n",
    "   ''' creates: explanation - a concise explanation of what the sql query does.\n",
    "                limitation - a concise explanation of the sql query by pointing out its limitations.\n",
    "                insight - insight from the result of the sql query.\n",
    "   '''\n",
    "   system_prompt = \"\"\"\n",
    "   You are an expert data analyst.\n",
    "\n",
    "   You are provided with the following SQL query:\n",
    "   {sql_query}.\n",
    "\n",
    "   Which yielded the following result:\n",
    "   {sql_query_result}.\n",
    "\n",
    "   Provide a structured analysis with three components:\n",
    "\n",
    "   Step 1: Explanation: A concise description of what the query outputs, in one short phrase. \n",
    "                   Do not include introductory words like \"The query\" or \"It outputs.\"\n",
    "\n",
    "   Step 2: Limitation: Inherent limitations or assumptions of the query based strictly on its structure and logic.\n",
    "                  Focus on:\n",
    "                  - How LIMIT, ORDER BY, GROUP BY, or JOINs may introduce assumptions\n",
    "                  - How filtering or aggregation logic may bias the output\n",
    "                  - Situations where the query might **return incomplete or misleading results due to logic only**\n",
    "                  - Cases where ORDER BY combined with LIMIT might exclude other rows with equal values (ties)\n",
    "\n",
    "                  Only describe things that follow **logically from the query**, not from the dataset itself.\n",
    "\n",
    "                  🚫 Do NOT mention:\n",
    "                  - speculate on what the user is trying to analyze\n",
    "                  - suggest what insights are missing\n",
    "                  - mention field names being correct or incorrect\n",
    "                  - mention data types, nulls, formatting, spelling, or schema correctness\n",
    "                  - mention what other attributes, columns, filters, or relationships \"could have\" been used\n",
    "                  - assume anything about the intent behind the query\n",
    "\n",
    "                  If the query has no structural limitations or assumptions, respond with exactly \"No comments for the query\".\n",
    "\n",
    "                  Respond in 1 to 3 concise sentences, or with the exact phrase above.\n",
    "   \n",
    "   Step 3: Insight: Key findings from the results, stating facts directly without technical terms.\n",
    "               - Include the limitations discovered in step 2, as long as it's different than \"No comments for the query\".\n",
    "               - Do not mention your subjective assessment over the results.\n",
    "               - Avoid technical terms like \"data\",\"dataset\",\"table\",\"list\",\"provided information\",\"query\" etc.\n",
    "   \"\"\"\n",
    "\n",
    "   prompt = ChatPromptTemplate.from_messages(('system',system_prompt))\n",
    "   chain = prompt | llm_fast.with_structured_output(QueryAnalysis)\n",
    "   return chain.invoke({'sql_query':sql_query,\n",
    "                        'sql_query_result':sql_query_result})   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc11ef",
   "metadata": {},
   "source": [
    "### Create query metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231961b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "from sqlglot import parse_one, exp\n",
    "\n",
    "def extract_metadata_from_sql_query(sql_query):\n",
    "   # returns a dictionary with parsed names of tables and columns used in filters, aggregations and groupings \n",
    "   \n",
    " ast = parse_one(sql_query)\n",
    "\n",
    " sql_query_metadata = {\n",
    "    \"tables\": [],\n",
    "    \"filters\": [],\n",
    "    \"aggregations\": [],\n",
    "    \"groupings\": []\n",
    " }\n",
    "\n",
    " # extract tables\n",
    " table_generator = ast.find_all(sqlglot.expressions.Table)\n",
    " for items in table_generator:\n",
    "    sql_query_metadata['tables'].append(items.sql())\n",
    " # remove dups\n",
    " sql_query_metadata['tables'] = list(dict.fromkeys(sql_query_metadata['tables']))\n",
    "\n",
    " # extract filters\n",
    " where_conditions = ast.find_all(sqlglot.expressions.Where)\n",
    " for item in where_conditions:\n",
    "  sql_query_metadata['filters'].append(item.this.sql())\n",
    "  # remove dups\n",
    " sql_query_metadata['filters'] = list(dict.fromkeys(sql_query_metadata['filters']))\n",
    "\n",
    " # extract aggregate functions\n",
    " funcs = ast.find_all(sqlglot.expressions.AggFunc)\n",
    " for item in funcs:\n",
    "  sql_query_metadata['aggregations'].append(item.sql())\n",
    " # remove dups\n",
    " sql_query_metadata['aggregations'] = list(dict.fromkeys(sql_query_metadata['aggregations']))\n",
    "\n",
    " # extract groupings\n",
    " groupings = ast.find_all(sqlglot.expressions.Group)\n",
    " for item in groupings:\n",
    "  groupings_flattened = item.flatten()\n",
    "  for item in groupings_flattened:\n",
    "    sql_query_metadata['groupings'].append(item.sql())\n",
    " # remove dups\n",
    " sql_query_metadata['groupings'] = list(dict.fromkeys(sql_query_metadata['groupings']))\n",
    "\n",
    " return {'tables':sql_query_metadata.get('tables'),\n",
    "         'filters':sql_query_metadata.get('filters'),\n",
    "         'aggregations':sql_query_metadata.get('aggregations'),\n",
    "         'groupings':sql_query_metadata.get('groupings'),\n",
    "          }\n",
    "\n",
    "def format_sql_metadata_explanation(tables:list=None, filters:list=None, aggregations:list=None, groupings:list=None,header :str='') -> str:\n",
    "    # creates a string explanation of the filters, tables, aggregations and groupings used by the query\n",
    "    explanation = header\n",
    "\n",
    "    if tables:\n",
    "        explanation += \"\\n\\n🧊 Tables: • \" + \" • \".join(tables)\n",
    "    if filters:\n",
    "        explanation += \"\\n\\n🔍 Filters applied: • \" + \" • \".join(filters)\n",
    "    if aggregations:\n",
    "        explanation += \"\\n\\n🧮 Aggregations: • \" + \" • \".join(aggregations)\n",
    "    if groupings:\n",
    "        explanation += \"\\n\\n📦 Groupings: • \" + \" • \".join(groupings)\n",
    "\n",
    "    return explanation.strip()\n",
    "\n",
    "def create_query_metadata(sql_query: str):\n",
    " \"\"\" creates an explanation for one single query \"\"\"\n",
    "\n",
    " metadata = extract_metadata_from_sql_query(sql_query)\n",
    " return format_sql_metadata_explanation(metadata['tables'],metadata['filters'],metadata['aggregations'],metadata['groupings'])\n",
    "\n",
    "\n",
    "def create_queries_metadata(sql_queries: list[dict]):\n",
    " \"\"\" creates an explanation for multiple queries: used in the generate_answer tool \"\"\"\n",
    "\n",
    " all_tables = []\n",
    " all_filters = []\n",
    " #all_aggregations = []\n",
    " #all_groupings = []\n",
    "\n",
    " for q in sql_queries: \n",
    "\n",
    "  metadata = extract_metadata_from_sql_query(q['query'])\n",
    "  all_tables.extend(metadata[\"tables\"])\n",
    "  all_filters.extend(metadata[\"filters\"])\n",
    "  #all_aggregations.extend(metadata[\"aggregations\"])\n",
    "  #all_groupings.extend(metadata[\"groupings\"])\n",
    "  \n",
    "  # include all metadata\n",
    "  #output = format_sql_metadata_explanation(all_tables,all_filters,all_aggregations,all_groupings,header='🔍 Filters applied:')\n",
    "\n",
    "  # include the default min/max feedback filters if feedback table has been used and was not filtered at all\n",
    "  if 'feedback' in all_tables and not any('feedback_date' in item for item in all_filters):\n",
    "     all_filters.append('feedback_date between 11/18/2002 and 09/12/2023')\n",
    "     output = format_sql_metadata_explanation(filters = all_filters,header='')\n",
    "  # include just the filters if there are any\n",
    "  elif all_filters:    \n",
    "     output = format_sql_metadata_explanation(filters = all_filters,header='')\n",
    "  # if no filters were applied, don't include other metadata for the sake of keeping the message simple\n",
    "  else:\n",
    "     output = ''   \n",
    "\n",
    " return output\n",
    "\n",
    "# use it like so:\n",
    "#sql_queries = [ \n",
    "#    {'query':'SELECT COUNT(DISTINCT company.company_name) FROM company;', 'result':''} ,\n",
    "#    {'query':'SELECT COUNT(DISTINCT feedback.feedback_id) FROM feedback;', 'result':''} \n",
    "#    ]\n",
    "#create_queries_metadata(sql_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d003cf",
   "metadata": {},
   "source": [
    "## Retrieve insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f0462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_or_retrieve_vector_store():\n",
    " global vector_store  \n",
    " if vector_store is None:\n",
    "    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    " return vector_store\n",
    "\n",
    "def parse_explanation(content:str):\n",
    "    ''' from a text with a format of Query Explanation: ... Query Insight: ...  parse just the explanation part '''\n",
    "    match = re.search(r\"Query Explanation:(.*?)Query Insight:\", content, re.DOTALL)\n",
    "    explanation = match.group(1).strip()  # removes leading/trailing whitespace including \\n\n",
    "    return explanation\n",
    "\n",
    "@tool\n",
    "def retrieve_insights(state:State):\n",
    "  ''' Searches the vector store for relevant past query insights with similarity > 0.6 and appends them to state['current_sql_queries'] '''\n",
    "  print(\"💭 Gathering my thoughts...\")\n",
    "  vector_store = create_or_retrieve_vector_store()\n",
    "  for query in enumerate(state['analytical_intent']):\n",
    "    query=query[1]\n",
    "    result = vector_store.similarity_search_with_score(query,k=3)\n",
    "    for doc,score in result:\n",
    "     if score >= 0.6:   \n",
    "      state['current_sql_queries'].append( {'query': doc.metadata.get('query'),\n",
    "                                     'explanation': parse_explanation(doc.page_content), \n",
    "                                     'result':doc.metadata.get('result'), \n",
    "                                     'insight': doc.metadata.get('insight'),\n",
    "                                     'metadata':doc.metadata.get('metadata')\n",
    "                                      } ) \n",
    "  # control flow\n",
    "  action = AgentAction(tool='retrieve_insights', tool_input='',log='tool ran successfully')\n",
    "  state['intermediate_steps'].append(action)  \n",
    "\n",
    "  return state  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1c989",
   "metadata": {},
   "source": [
    "## Execute (& refine) sql query and stores result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f766366d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# the function checks if the query output exceeds context window limit and if yes, send the query for refinement\n",
    "\n",
    "from langchain_community.tools import QuerySQLDataBaseTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from typing import Iterator\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "db = SQLDatabase(engine)\n",
    "\n",
    "def execute_sql_query(state:State):\n",
    "  \"\"\" executes the sql query and retrieve the result \"\"\"\n",
    "  \n",
    "  print(\"⚙️ Analysing results...\")\n",
    "  for query_index, q in enumerate(state['current_sql_queries']):\n",
    "     \n",
    "    if state['current_sql_queries'][query_index]['result'] == '':    \n",
    "     sql_query = q['query'] \n",
    "    \n",
    "     # refine the query 3 times if necessary.\n",
    "     for i in range(3):\n",
    "\n",
    "       sql_query_result = QuerySQLDataBaseTool(db=db).invoke(sql_query)\n",
    "\n",
    "       # if the sql query does not exceed output context window return its result\n",
    "       if not check_if_exceed_maximum_context_limit(sql_query_result):\n",
    "         analysis = create_query_analysis(sql_query, sql_query_result)\n",
    "         sql_query_metadata = create_query_metadata(sql_query)   \n",
    "\n",
    "         # Update state\n",
    "         state['current_sql_queries'][query_index]['result'] = sql_query_result\n",
    "         state['current_sql_queries'][query_index]['insight'] = analysis['insight']\n",
    "         state['current_sql_queries'][query_index]['query'] = sql_query\n",
    "         state['current_sql_queries'][query_index]['metadata'] = sql_query_metadata\n",
    "         state['current_sql_queries'][query_index]['explanation'] = analysis['explanation']   \n",
    "\n",
    "         # add the queries to vector store\n",
    "         vector_store = create_or_retrieve_vector_store()\n",
    "         doc = [Document(\n",
    "              id=len(vector_store.store)+1,\n",
    "              page_content=f\"Query Explanation:\\n{analysis['explanation'] }\\n\\Query Insight:{analysis['insight']}\",\n",
    "              metadata={\"query\": sql_query,\n",
    "                        \"result\": sql_query_result,\n",
    "                        \"insight\": analysis['insight'],\n",
    "                        \"metadata\": sql_query_metadata\n",
    "                        })]\n",
    "         vector_store.add_documents(documents=doc)                                                          \n",
    "         break\n",
    "\n",
    "       # if the sql query exceeds output context window and there is more room for iterations, refine the query\n",
    "       else:\n",
    "        print(f\"🔧 Refining query {query_index+1}/{len(state['current_sql_queries'])} as its output its too large...\")\n",
    "        sql_query = refine_sql_query(state['analytical_intent'],sql_query,state['objects_documentation'])['query']\n",
    "\n",
    "       # if there is no more room for sql query iterations and the result still exceeds context window, throw a message\n",
    "     else:\n",
    "        print(f\"⚠️ Query result too large after 3 refinements.\")\n",
    "        state['current_sql_queries'][query_index]['result'] = 'Query result too large after 3 refinements.'\n",
    "        state['current_sql_queries'][query_index]['explanation'] = \"Refinement failed.\"\n",
    "      \n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e660ade",
   "metadata": {},
   "source": [
    "### Query Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7489351",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class OutputAsASingleQuery(TypedDict):\n",
    "  \"\"\" generated sql query \"\"\"\n",
    "  query: Annotated[str,...,\"the generated sql query\"]\n",
    "\n",
    "def refine_sql_query(analytical_intent: str, sql_query: str, objects_documentation: int):\n",
    " \"\"\" refines the sql query so that its output tokens do not exceed the maximum context limit \"\"\"\n",
    "\n",
    " system_prompt = \"\"\"\n",
    "  As a sql expert, your task is to optimize a sql query (in snowflake dialect) that returns more than 20 rows or exceeds the token limit.\n",
    "  \n",
    "  You are trying to answer the following analytical intent: {analytical_intent}.\n",
    "  Sql query to optimize: {sql_query}.\n",
    "\n",
    "  Use the optimization guidelines below that are most appropriate to reduce output size while maximizing the insights for the analytical intent.\n",
    "\n",
    "  If you use new column or table names, use just the ones from the following database schema: {objects_documentation}.\n",
    " \n",
    "  *** Optimization Guidelines ***  \n",
    "  \n",
    "  A. Apply aggregation functions instead of returning a list of records.\n",
    "      Example: - Analytical intent: \"number of distinct ids in table where column equals 5\"\n",
    "               - Original sql query: \"SELECT DISTINCT id FROM table WHERE rating = 5;\"\n",
    "\t\t\t         - Refined sql query: \"SELECT COUNT(DISTINCT id) FROM table WHERE rating = 5;\"\n",
    "\t\t\t\n",
    "  B. Group the data at a higher granularity.\n",
    "     Example: If sql query shows data by days, group by months and return last N months.\n",
    "  \n",
    "  C. Group the data in buckets.\n",
    "      Example: - Analytical intent: \"Analyze the relationship between products.product_price and products.product_average-rating in the products table to determine if product price influences average rating.\"\n",
    "               - Original sql query: \"SELECT product_price, product_average_rating FROM products GROUP BY product_price, product_average_rating\"\n",
    "\t\t\t         - Refined sql query: \"SELECT \n",
    "                                         CASE \n",
    "                                             WHEN product_price < 10 THEN '<$10'\n",
    "                                             WHEN product_price >= 10 AND product_price < 50 THEN '$10–$50'\n",
    "                                             ELSE '$50+'\n",
    "                                             END AS price_bucket,\n",
    "                                         CASE \n",
    "                                             WHEN product_price < 10 THEN 1\n",
    "                                             WHEN product_price >= 10 AND product_price < 50 THEN 2\n",
    "                                             ELSE 3\n",
    "                                             END AS price_bucket_sort,                                             \n",
    "                                         ROUND(AVG(product_average_rating), 2) AS avg_rating,\n",
    "                                         COUNT(*) AS product_count\n",
    "                                   FROM products\n",
    "                                   GROUP BY price_bucket\n",
    "                                   ORDER BY price_bucket_sort;\"\n",
    "   \n",
    "  \n",
    "  D. Apply time-based filters.\n",
    "     Example: - Show records for the last 3 months.\n",
    "              - filter the entire dataset for a single client.\n",
    "  \n",
    "  E. Show top records.\n",
    "     Provide a snapshot of data by retrieving maximum 20 rows and 5 columns.\n",
    "     Example: The Analytical intent is: \"average sale per customer\", but there are too many customers, so show top N.\n",
    "                       - Original sql query: \"SELECT customer_name, avg(sale) as avg_sale from sales group by customer_name\"\n",
    "\t\t\t                 - Refined sql query: \"SELECT customer_name, avg(sale) as avg_sale from sales group by customer_name ORDER BY avg_sale desc limit 10;\"\n",
    "  \"\"\"\n",
    " \n",
    " prompt = ChatPromptTemplate.from_messages(('system',system_prompt))\n",
    " chain = prompt | llm.with_structured_output(OutputAsASingleQuery)\n",
    "\n",
    " sql_query = chain.invoke({'analytical_intent': analytical_intent,\n",
    "               'sql_query':sql_query,\n",
    "               'objects_documentation':objects_documentation}\n",
    "               )\n",
    " return sql_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952f76b",
   "metadata": {},
   "source": [
    "## Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ae8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a function that generates the agent answer based on sql query result\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def format_sql_query_results_for_prompt (sql_queries : list[dict]) -> str:\n",
    "    \n",
    "    formatted_queries = []\n",
    "    for query_index,q in enumerate(sql_queries):\n",
    "        block = f\"Insight {query_index+1}:\\n{q['insight']}\\n\\nRaw Result of insight {query_index+1}:\\n{q['result']}\"\n",
    "        formatted_queries.append(block)\n",
    "    return \"\\n\\n\".join(formatted_queries)\n",
    "\n",
    "@tool\n",
    "def generate_answer(state:State):\n",
    "  \"\"\" generates the AI answer taking into consideration the explanation and the result of the sql query that was executed \"\"\"\n",
    "  \n",
    "  scenario = state['generate_answer_details']['scenario']\n",
    "\n",
    "  # response guidelines to be added at the end of every prompt\n",
    "  response_guidelines = '''\n",
    "  Response guidelines:\n",
    "  - Respond in clear, non-technical language. \n",
    "  - Be concise. \n",
    "\n",
    "  Use these methods at the right time, optionally and not too much, keep it simple and conversational:\n",
    "\n",
    "  - If the question is smart, reinforce the user’s question to build confidence. \n",
    "    Example: “Great instinct to ask that - it’s how data-savvy pros think!”\n",
    "\n",
    "  If the context allows, suggest max 2 next steps to explore further. \n",
    "  Suggest next steps that can only be achieved with the data from these tables and columns you have access to:\n",
    "  {objects_documentation}\n",
    "  \n",
    "  Example of next steps:\n",
    "  - Trends over time:\n",
    "    Example: \"Want to see how this changed over time?\".\n",
    "\n",
    "  - Drill-down suggestions:\n",
    "    Example: “Would you like to explore this by brand or price tier?”\n",
    "\n",
    "  - Top contributors to a trend:\n",
    "    Example: “Want to see the top 5 products that drove this increase in satisfaction?”\n",
    "\n",
    "  - Explore a possible cause:\n",
    "    Example: “Curious if pricing could explain the drop? I can help with that.”\n",
    "\n",
    "  Close the prompt in one of these ways:\n",
    "  A. If you suggest next steps, ask the user which option prefers.\n",
    "  B. Use warm, supportive closing that makes the user feel good. \n",
    "    Example: “Keep up the great work!”, “Have a great day ahead!”.\n",
    "  '''\n",
    "  \n",
    "  # write the prompts for each scenario\n",
    "  prompts = { \n",
    "              # no insights needed (with response guidelines)\n",
    "              'No New Insights Needed':\"\"\" You are a decision support consultant helping users become more data-driven.\n",
    "     Continue the conversation from the last user prompt. \n",
    "     \n",
    "     Conversation history:\n",
    "     {messages_log}.\n",
    "\n",
    "    Last user prompt:\n",
    "    {question}.  \n",
    "     \"\"\".strip() + '\\n\\n' + response_guidelines.strip(),\n",
    "             # analytical intent extracted (with response guidelines)\n",
    "             'Analytical Intent Extracted':\"\"\" You are a decision support consultant helping users become more data-driven.\n",
    "     Continue the conversation from the last user prompt. \n",
    "     \n",
    "     Conversation history:\n",
    "     {messages_log}.\n",
    "\n",
    "     Last user prompt:\n",
    "     {question}.  \n",
    "\n",
    "     Use both the raw SQL results and the extracted insights below to form your answer: {insights}. \n",
    "     \n",
    "     Include all details from these insights.\n",
    "     \"\"\".strip() + '\\n\\n' + response_guidelines.strip(),\n",
    "             # info not available (with response guidelines)\n",
    "             'Info Not Available':\"\"\" You are a decision support consultant helping users become more data-driven.\n",
    "     Continue the conversation from the last user prompt. \n",
    "     \n",
    "     Conversation history:\n",
    "     {messages_log}.\n",
    "\n",
    "     Last user prompt:\n",
    "     {question}.\n",
    "        \n",
    "     Unfortunately, the requested information from last prompt is not available in our database. Here are the details: {notes}.\n",
    "        \n",
    "     Use the response guidelines below to explain what information is not available by suggesting alternative analyses that can be performed with the available data.\n",
    "     \"\"\".strip() + '\\n\\n' + response_guidelines.strip(), \n",
    "             # analytical intent ambiguous\n",
    "               'Analytical Intent Ambiguous':\"\"\"You are a decision support consultant helping users become more data-driven.\n",
    "        \n",
    "      Continue the conversation from the last user prompt. \n",
    "     \n",
    "      Conversation history:\n",
    "      {messages_log}.\n",
    "\n",
    "      Last user prompt:\n",
    "      {question}.\n",
    "        \n",
    "      The last user prompt could be interpreted in multiple ways. Here's what makes it ambiguous: {notes}.\n",
    "        \n",
    "      Acknowledge what makes the question ambiguous, present different options as possible interpretations and ask the user to specify which analysis it wants.\n",
    "\n",
    "      Respond in clear, non-technical language. Be concise. \n",
    "       \"\"\" }\n",
    "\n",
    "  # create prompt temaplate\n",
    "  prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(\"messages_log\"),('system',prompts[scenario])] )\n",
    "  llm_answer_chain = prompt | llm\n",
    "\n",
    "  if scenario == 'Analytical Intent Extracted': # show filters\n",
    "    final_answer_chain = { 'llm_answer': llm_answer_chain\n",
    "                         ,'input_state': RunnablePassthrough()  \n",
    "                           } | RunnableLambda (lambda x: { 'ai_message': AIMessage( content = f\"{x['llm_answer'].content.strip()}\\n\\n{create_queries_metadata(x['input_state']['current_sql_queries'])}\"\n",
    "                                                                         ,response_metadata = x['llm_answer'].response_metadata  ) } ) \n",
    "  else: # filters not necessary\n",
    "    final_answer_chain = { 'llm_answer': llm_answer_chain\n",
    "                          , 'input_state': RunnablePassthrough() \n",
    "                          } | RunnableLambda (lambda x: { 'ai_message': AIMessage( content = f\"{x['llm_answer'].content}\"\n",
    "                                                                        ,response_metadata = x['llm_answer'].response_metadata  ) } )  \n",
    "    \n",
    "\n",
    "  # execute the chain\n",
    "  invoke_params = { 'messages_log':state['messages_log'],\n",
    "                    'question':state['current_question'],\n",
    "                    'objects_documentation':state['objects_documentation'] }\n",
    "\n",
    "  if scenario == 'Analytical Intent Extracted':\n",
    "    invoke_params.update( {'insights': format_sql_query_results_for_prompt(state['current_sql_queries']),\n",
    "                           'current_sql_queries': state['current_sql_queries'] })\n",
    "  elif scenario in ['Info Not Available','Analytical Intent Ambiguous']:\n",
    "    invoke_params['notes'] = state['generate_answer_details']['notes']\n",
    "\n",
    "  result = final_answer_chain.invoke(invoke_params)\n",
    "  ai_msg = result['ai_message']\n",
    "\n",
    "  # Add token count for SQL metadata if applicable\n",
    "  if scenario == 'Analytical Intent Extracted':\n",
    "    explanation_token_count = llm.get_num_tokens(create_queries_metadata(state['current_sql_queries']))\n",
    "    ai_msg.response_metadata['token_usage']['total_tokens'] += explanation_token_count\n",
    "  \n",
    "  # Update state (common for all scenarios)\n",
    "  state['llm_answer'] = ai_msg\n",
    "  state['messages_log'].append(HumanMessage(state['current_question']))\n",
    "  state['messages_log'].append(ai_msg) \n",
    "\n",
    "  print(\"\\n📣 Final Answer:\\n\")\n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68cae6",
   "metadata": {},
   "source": [
    "### Manage memory and chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f942bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_memory_chat_history(state:State):\n",
    "    \"\"\" Manages the chat history so that it does not become too large in terms of output tokens.\n",
    "    Specifically, it checks if the chat history is larger than 1000 tokens. If yes, keep just the last 4 pairs of human prompts and AI responses, and summarize the older messages.\n",
    "    Additionally, check if the logs of sql queries is larger than 20 entries. If yes, delete the older records. \"\"\"           \n",
    "\n",
    "    tokens_chat_history = state['messages_log'][-1].response_metadata.get('token_usage', {}).get('total_tokens', 0) if state['messages_log'] else 0    \n",
    "\n",
    "    if tokens_chat_history >= 1000 and len(state['messages_log']) > 4:\n",
    "        message_history_to_summarize = [msg.content for msg in state['messages_log'][:-4]]\n",
    "        prompt = ChatPromptTemplate.from_messages( [('user', 'Distill the below chat messages into a single summary paragraph.The summary paragraph should have maximum 400 tokens.Include as many specific details as you can.Chat messages:{message_history_to_summarize}') ])\n",
    "        runnable = prompt | llm_fast # use the cheap model\n",
    "        chat_history_summary = runnable.invoke({'message_history_to_summarize':message_history_to_summarize})\n",
    "        last_4_messages = state['messages_log'][-4:]\n",
    "        state['messages_log'] = [chat_history_summary] +[*last_4_messages]\n",
    "    else:\n",
    "        state['messages_log'] = state['messages_log']\n",
    "\n",
    "    # Truncate SQL logs to the most recent 20\n",
    "    #if len(state['log_sql_queries']) > 20:\n",
    "    #    state['log_sql_queries']= state['log_sql_queries'][-20:]    \n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e8d9d",
   "metadata": {},
   "source": [
    "## Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24098660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_tool(state:State):\n",
    "  ''' creates a list of actions taken by the agent from the intermediate steps '''  \n",
    "  nr_executions_extract_analytical_intent = 0\n",
    "  nr_executions_retrieve_insights = 0\n",
    "  nr_executions_create_sql_query_or_queries = 0\n",
    "  \n",
    "  for index,action in enumerate(state['intermediate_steps']):\n",
    "      \n",
    "    if action.tool == 'extract_analytical_intent' and action.log == 'tool ran successfully':\n",
    "      nr_executions_extract_analytical_intent +=1\n",
    "    \n",
    "    if action.tool == 'retrieve_insights' and action.log == 'tool ran successfully':\n",
    "      nr_executions_retrieve_insights +=1\n",
    "\n",
    "    if action.tool == 'create_sql_query_or_queries' and action.log == 'tool ran successfully':\n",
    "      nr_executions_create_sql_query_or_queries +=1\n",
    "\n",
    "  if nr_executions_extract_analytical_intent == 0:\n",
    "    next_tool = 'extract_analytical_intent' \n",
    "  elif nr_executions_retrieve_insights == nr_executions_create_sql_query_or_queries == nr_executions_extract_analytical_intent == 1:\n",
    "    next_tool = 'generate_answer'\n",
    "  elif nr_executions_extract_analytical_intent > 0 and nr_executions_retrieve_insights > 0 and nr_executions_create_sql_query_or_queries == 0:\n",
    "    next_tool = 'create_sql_query_or_queries'  \n",
    "\n",
    "  return next_tool\n",
    "\n",
    "class OrchestratorNextStep(TypedDict):\n",
    "  ''' indication of the next step to be performed by the agent '''\n",
    "  next_step: Annotated[Literal[\"No New Insights Needed\", \"Info Not Available\",\"Insights Are Enough\",\"Need Insights\"],\"indication of the next step to be performed by the agent\"] \n",
    "\n",
    "def orchestrator(state:State):\n",
    "  ''' Function that decides which tools to use '''\n",
    "\n",
    "  # if all tools for retrieving insights have been used, go directly to answer.\n",
    "  next_tool = get_next_tool(state)\n",
    "  if next_tool == 'generate_answer':\n",
    "     action = AgentAction(tool='generate_answer', tool_input='', log='')\n",
    "     state['intermediate_steps'].append(action)\n",
    "     return state\n",
    "\n",
    "  else:\n",
    "    system_prompt = f\"\"\"You are a decision support consultant helping users make data-driven decisions.\n",
    "\n",
    "    Your task is to decide the next action for this question: {{question}}.\n",
    "\n",
    "    Conversation history: {{messages_log}}. \n",
    "    Current insights: \"{{insights}}\".\n",
    "    Database schema: {{objects_documentation}}\n",
    "\n",
    "    Decision process:  \n",
    "\n",
    "    Step 1. Check if question is non-analytical or already answered:\n",
    "       - If question is just pleasantries (\"thank you\", \"hello\", \"how are you\") → \"No New Insights Needed\"\n",
    "       - If the same question was already answered in conversation history → \"No New Insights Needed\"\n",
    "\n",
    "    Step 2. Check if requested data exists in schema:\n",
    "      - If the user asks for data/metrics not available in the database schema → \"Info Not Available\"\n",
    "\n",
    "    Step 3. Check if we have sufficient insights to answer:\n",
    "       - If current insights directly answer the user's question → \"Insights Are Enough\"\n",
    "\n",
    "    Step 4. Otherwise -> \"Need Insights\".  \n",
    "    \"\"\"\n",
    "    \n",
    "    sys_prompt_notes = \"\"\"\n",
    "    Conversation history:\n",
    "    {messages_log}.  \n",
    "\n",
    "    Last user prompt:\n",
    "    {question}. \n",
    "  \n",
    "    The user asked for data that is not available in the database schema.\n",
    "    Write a sentence suggesting an analysis with the existing schema.\n",
    "    Database schema:\n",
    "    {objects_documentation}.\n",
    "\n",
    "    Be short, concise, explain in simple, non-technical language.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([('system', system_prompt)])\n",
    "\n",
    "    chain = prompt | llm_fast.with_structured_output(OrchestratorNextStep)\n",
    "    orchestrator = chain.invoke({'messages_log':extract_msg_content_from_history(state['messages_log']),\n",
    "                         'question': state['current_question'], \n",
    "                         'insights': format_sql_query_results_for_prompt(state['current_sql_queries']),\n",
    "                         'objects_documentation':state['objects_documentation']\n",
    "                         })                   \n",
    "\n",
    "    # update the state if no new insights needed\n",
    "    if orchestrator['next_step']  in ['No New Insights Needed','Insights Are Enough']: \n",
    "        state['generate_answer_details'].update({'scenario':orchestrator['next_step'],'notes':None})\n",
    "        tool_name = 'generate_answer'     \n",
    "    # update the state if Info Not Available\n",
    "    elif orchestrator['next_step']  == 'Info Not Available': \n",
    "        # suggest possible analysis\n",
    "        prompt_notes = ChatPromptTemplate.from_messages([('system', sys_prompt_notes)]) \n",
    "        chain_4= prompt_notes | llm_fast\n",
    "        notes_text = chain_4.invoke({'messages_log':extract_msg_content_from_history(state['messages_log']),\n",
    "                         'question': state['current_question'], \n",
    "                         'objects_documentation':state['objects_documentation']\n",
    "                                   })\n",
    "        state['generate_answer_details'].update({'scenario':orchestrator['next_step'],'notes':notes_text.content})\n",
    "        tool_name = 'generate_answer' \n",
    "    else:\n",
    "      tool_name = get_next_tool(state)  \n",
    "\n",
    "    # log next tool to call\n",
    "    action = AgentAction( tool=tool_name, tool_input='', log = '' )\n",
    "    state['intermediate_steps'].append(action)  \n",
    "    return state     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce9f3c",
   "metadata": {},
   "source": [
    "### Run control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a697fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the nodes\n",
    "\n",
    "def run_control_flow(state:State):\n",
    "    ''' Based on the last tool name stored in intermediate_steps (generated by the orchestrator), it executes the next node that will trigger the control flow '''\n",
    "    \n",
    "    # get the next tool to execute by looking in the last tool_name in the intermediate steps\n",
    "    tool_name = state['intermediate_steps'][-1].tool\n",
    "    \n",
    "    # extract_analytical_intent\n",
    "    if tool_name == 'extract_analytical_intent':\n",
    "      state = extract_analytical_intent.invoke({'state':state})  \n",
    "\n",
    "    # retrieve_insights\n",
    "    if tool_name == 'retrieve_insights':\n",
    "      state = retrieve_insights.invoke({'state':state})  \n",
    "\n",
    "    # creating & executing new queries\n",
    "    elif tool_name == 'create_sql_query_or_queries':\n",
    "      state = create_sql_query_or_queries.invoke({'state':state})\n",
    "      execute_sql_query(state)\n",
    "\n",
    "    # generate answer & manage chat history.\n",
    "    elif tool_name == 'generate_answer':  \n",
    "      state = generate_answer.invoke({'state':state}) \n",
    "      manage_memory_chat_history(state)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7ce8d",
   "metadata": {},
   "source": [
    "## Assemble graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23aa439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble graph\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# function to reset the state current queries (to add in the start of graph execution)\n",
    "def reset_state(state:State):\n",
    "    state['current_sql_queries'] = []\n",
    "    state['intermediate_steps'] = []\n",
    "    state['llm_answer'] = AIMessage(content='')\n",
    "    state['generate_answer_details'] = {}\n",
    "    state['analytical_intent'] = []\n",
    "    state['objects_documentation'] = \"\"\"\n",
    "     Table company: List of public companies. Granularity is company-name. Column (prefixed with table name):\n",
    "     company.company-name: the name of the public company.\n",
    "     company.annual_revenue_usd: revenue in last 12 months ($).\n",
    "\n",
    "     Table feedback: Feedbacks given by clients to products. Granularity is feedback. Key is feedback_id. Columns (prefixed with table name):\n",
    "     feedback.feedback_id: id of the feedback.\n",
    "     feedback.feedback_date: date of feedback.\n",
    "     feedback.product_id: id of the product the feedback was given for.\n",
    "     feedback.product_company_name: company owning the product.\n",
    "     feedback.feedback_text: the text of the feedback.\n",
    "     feedback.feedback_rating: rating of the feedback from 1 to 5, 5 being the highest score.\n",
    "\n",
    "     Table products: Shows product metadata. Granularity is product. Key is product_id. Columns (prefixed with table name):\n",
    "     products.product_id: id of the product.\n",
    "     products.product_name: name of the product.\n",
    "     products.product_brand: the brand under which the product was presented.\n",
    "     products.product_manufacturer: product manufacturer.\n",
    "     products.product_company_name: company owning the product.\n",
    "     products.product_price: price of the product at crawling time.\n",
    "     products.product_average_rating: average ratings across all feedbacks for the product, at crawling time.\n",
    "\n",
    "     Table company -> column company_name relates to table feedback -> column product_company_name\n",
    "     Table products -> column product_company_name relates to table feedback -> column product_company-name\n",
    "     Table feedback -> column product_id relates to table products -> column product_id\n",
    "     \"\"\"\n",
    "    return state\n",
    "\n",
    "def router(state:State):\n",
    "    # returns the tool name to use\n",
    "    return state['intermediate_steps'][-1].tool\n",
    "\n",
    "graph= StateGraph(State)\n",
    "graph.add_node(\"reset_state\",reset_state)\n",
    "graph.add_node(\"orchestrator\",orchestrator)\n",
    "\n",
    "# here you add the node corresponding to the first tool of each control flow, as the subsequent tools are run by the run_control_flow node\n",
    "graph.add_node(\"extract_analytical_intent\",run_control_flow)\n",
    "graph.add_node(\"retrieve_insights\",run_control_flow)\n",
    "graph.add_node(\"create_sql_query_or_queries\",run_control_flow)\n",
    "graph.add_node(\"generate_answer\",run_control_flow)\n",
    "\n",
    "# starting the agent\n",
    "graph.add_edge(START,\"reset_state\")\n",
    "graph.add_edge(\"reset_state\",\"orchestrator\")\n",
    "graph.add_conditional_edges(source='orchestrator',path=router)\n",
    "graph.add_conditional_edges(source='extract_analytical_intent',path=router)\n",
    "\n",
    "# here you add a link from each the control flow node back to the orchestator - except for the generate_answer node.\n",
    "graph.add_edge(\"retrieve_insights\",\"orchestrator\")\n",
    "graph.add_edge(\"create_sql_query_or_queries\",\"orchestrator\")\n",
    "\n",
    "# last control flow is generate_answer\n",
    "graph.add_edge(\"generate_answer\",END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8b960",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcd929",
   "metadata": {},
   "source": [
    "\n",
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new conversation\n",
    "\n",
    "question = 'How many products have a rating of 5?'\n",
    "messages_log = []\n",
    "\n",
    "initial_dict = {'objects_documentation':objects_documentation,\n",
    "     'messages_log': messages_log,\n",
    "     'intermediate_steps':[],\n",
    "     'analytical_intent': [],\n",
    "     'current_question':question,\n",
    "     'current_sql_queries': [],\n",
    "     'generate_answer_details': {},\n",
    "     'llm_answer': AIMessage(content='')\n",
    "     }\n",
    "     \n",
    "vector_store = None  # reset vector store\n",
    "config, thread_id = create_config('Run Agent',True)\n",
    "\n",
    "result = graph.invoke(initial_dict, config = config)\n",
    "print(result['llm_answer'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "question = 'I would like to explore option 3'\n",
    "config, _ = create_config('Run Agent',False,thread_id)\n",
    "result = graph.invoke({\n",
    "    'current_question': question\n",
    "}, config=config)\n",
    "\n",
    "print(result['llm_answer'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b21e19",
   "metadata": {},
   "source": [
    "### Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b81e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💭 Gathering my thoughts...\n",
      "✅ SQL queries created:1\n",
      "⚙️ Analysing results...\n",
      "\n",
      "📣 Final Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_store = None  # reset vector store\n",
    "question = 'How many products have a rating of 5?'\n",
    "test_state = {\n",
    "'objects_documentation':objects_documentation,\n",
    "'messages_log':[],\n",
    "'intermediate_steps' : [],\n",
    "'analytical_intent': [],\n",
    "'current_question':question,\n",
    "'current_sql_queries': [],\n",
    "'generate_answer_details': {},\n",
    "'llm_answer': AIMessage(content='')\n",
    "}\n",
    "orchestrator(test_state)\n",
    "test_state = run_control_flow(test_state) # extract_analytical_intent\n",
    "test_state = run_control_flow(test_state) # retrieve_insights\n",
    "orchestrator(test_state)\n",
    "test_state = run_control_flow(test_state) # create sql query + execute sql query\n",
    "orchestrator(test_state)\n",
    "test_state = run_control_flow(test_state) # generate answer + manage memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f639dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = None  # reset vector store\n",
    "question = 'How many products have a rating of 5?'\n",
    "test_state = {\n",
    "'objects_documentation':objects_documentation,\n",
    "'messages_log':[],\n",
    "'intermediate_steps' : [],\n",
    "'analytical_intent': [],\n",
    "'current_question':question,\n",
    "'current_sql_queries': [],\n",
    "'generate_answer_details': {},\n",
    "'llm_answer': AIMessage(content='')\n",
    "}\n",
    "orchestrator(test_state)\n",
    "test_state = run_control_flow(test_state) # extract_analytical_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator(test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c03ec679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objects_documentation': 'Table company: List of public companies. Granularity is company-name. Column (prefixed with table name):\\n     company.company-name: the name of the public company.\\n     company.annual_revenue_usd: revenue in last 12 months ($).\\n\\n     Table feedback: Feedbacks given by clients to products. Granularity is feedback. Key is feedback_id. Columns (prefixed with table name):\\n     feedback.feedback_id: id of the feedback.\\n     feedback.feedback_date: date of feedback.\\n     feedback.product_id: id of the product the feedback was given for.\\n     feedback.product_company_name: company owning the product.\\n     feedback.feedback_text: the text of the feedback.\\n     feedback.feedback_rating: rating of the feedback from 1 to 5, 5 being the highest score.\\n\\n     Table products: Shows product metadata. Granularity is product. Key is product_id. Columns (prefixed with table name):\\n     products.product_id: id of the product.\\n     products.product_name: name of the product.\\n     products.product_brand: the brand under which the product was presented.\\n     products.product_manufacturer: product manufacturer.\\n     products.product_company_name: company owning the product.\\n     products.product_price: price of the product at crawling time.\\n     products.product_average_rating: average ratings across all feedbacks for the product, at crawling time.\\n\\n     Table company -> column company_name relates to table feedback -> column product_company_name\\n     Table products -> column product_company_name relates to table feedback -> column product_company-name\\n     Table feedback -> column product_id relates to table products -> column product_id',\n",
       " 'messages_log': [],\n",
       " 'intermediate_steps': [AgentAction(tool='extract_analytical_intent', tool_input='', log=''),\n",
       "  AgentAction(tool='extract_analytical_intent', tool_input='', log='tool ran successfully'),\n",
       "  AgentAction(tool='generate_answer', tool_input='', log='')],\n",
       " 'analytical_intent': ['Count the number of products in the products table where products.product_average_rating = 5.',\n",
       "  'Count the number of distinct products in the products table that have at least one feedback in the feedback table with feedback.feedback_rating = 5.',\n",
       "  'Count the number of products in the products table where the average of feedback.feedback_rating from the feedback table, grouped by feedback.product_id, is equal to 5.'],\n",
       " 'current_question': 'How many products have a rating of 5?',\n",
       " 'current_sql_queries': [],\n",
       " 'generate_answer_details': {'scenario': 'Analytical Intent Ambiguous',\n",
       "  'notes': 'The question \"How many products have a rating of 5?\" is unclear because it can be interpreted in different ways, leading to different answers. Here are the alternatives:\\n\\n1. Count the number of products in the products table where the average rating is exactly 5.\\n2. Count the number of distinct products that have received at least one feedback with a rating of 5.\\n3. Count the number of products where the average of all feedback ratings is exactly 5.\\n\\nEach interpretation uses different criteria to determine what qualifies as a \"rating of 5.\"'},\n",
       " 'llm_answer': AIMessage(content='', additional_kwargs={}, response_metadata={})}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "67585d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the conversation\n",
    "test_state['current_question'] = 'Yep, I would like to see how these ratings changed over time per company'\n",
    "test_state['intermediate_steps'] = []\n",
    "test_state['current_sql_queries'] = []\n",
    "test_state['generate_answer_details'] = {}\n",
    "test_state['analytical_intent'] = []\n",
    "test_state['llm_answer'] = AIMessage(content='')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "951c4e1ccc8e7dc066b7b3456b4d29f8a6c8c8949bd81a565897b5da2568416e"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
