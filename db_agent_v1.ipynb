{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc147a51",
   "metadata": {},
   "source": [
    "### Import feedbacks.db file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test access to db file: import db tables into data frames and select by the column names\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import uuid\n",
    "\n",
    "engine = create_engine('sqlite:///feedbacks_db.db')\n",
    "inspector = inspect(engine)\n",
    "\n",
    "df_company = pd.read_sql_query('SELECT company_name,annual_revenue_usd FROM company', engine)\n",
    "df_feedback = pd.read_sql_query('SELECT feedback_id,feedback_date,product_id,product_company_name,feedback_text,\"feedback_rating\" FROM feedback', engine)\n",
    "df_products = pd.read_sql_query('SELECT product_id,product_name,product_brand,product_manufacturer,product_company_name,product_price,product_average_rating FROM products', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4008f31",
   "metadata": {},
   "source": [
    "### Instantiate chat model (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8de278ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain, langgraph, langchain_openai, langsmith\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ['LANGSMITH_API_KEY'] = LANGSMITH_API_KEY\n",
    "os.environ['LANGSMITH_TRACING'] = \"true\"\n",
    "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "langsmith_project_name = \"db_agent_v1\"\n",
    "os.environ['LANGSMITH_PROJECT'] = langsmith_project_name\n",
    "\n",
    "# Set up LangSmith tracer manually\n",
    "tracer = LangChainTracer(project_name=langsmith_project_name)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o',temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fc53d",
   "metadata": {},
   "source": [
    "### Create config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1b11c69a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def create_config(run_name: str, is_new_thread_id: bool = False, thread_id: str = None):\n",
    "    \"\"\"\n",
    "    Create a config dictionary for LCEL runnables.\n",
    "    Includes LangSmith run tracing and optional thread_id management.\n",
    "\n",
    "    Args:\n",
    "        run_name (str): Descriptive run name shown in LangSmith.\n",
    "        is_new_thread_id (bool): Whether to generate a new thread_id.\n",
    "        thread_id (str): Optionally provide an existing thread_id to reuse.\n",
    "\n",
    "    Returns:\n",
    "        dict: Config dictionary with callbacks, run_name, and thread_id.\n",
    "\n",
    "    Use it like so (example): \n",
    "        config, thread_id = create_config('create_sql_query_or_queries', True) (start a new thread)\n",
    "        config, _ = create_config('generate_answer', False, thread_id) (re-use same thread)\n",
    "    \"\"\"\n",
    "\n",
    "    time_now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    full_run_name = f\"{run_name} {time_now}\"\n",
    "    if is_new_thread_id or not thread_id:\n",
    "        thread_id = str(uuid.uuid4())\n",
    "\n",
    "    config={'callbacks': [tracer],\n",
    "            'run_name': full_run_name,\n",
    "            'configurable' : { 'thread_id':thread_id }\n",
    "            }\n",
    "\n",
    "    return config,thread_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57674e30",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c97c018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "objects_documentation = \"\"\"\n",
    "  Table company: List of public companies. Granularity is company-name. Column (prefixed with table name):\n",
    "  company.company-name: the name of the public company.\n",
    "  company.annual_revenue_usd: revenue in last 12 months ($).\n",
    "\n",
    "  Table feedback: Feedbacks given by clients to products. Granularity is feedback. Key is feedback_id. Columns (prefixed with table name):\n",
    "  feedback.feedback_id: id of the feedback.\n",
    "  feedback.feedback_date: date of feedback.\n",
    "  feedback.product_id: id of the product the feedback was given for.\n",
    "  feedback.product_company_name: company owning the product.\n",
    "  feedback.feedback_text: the text of the feedback.\n",
    "  feedback.feedback_rating: rating of the feedback from 1 to 5, 5 being the highest score.\n",
    "\n",
    "  Table products: Shows product metadata. Granularity is product. Key is product_id. Columns (prefixed with table name):\n",
    "  products.product_id: id of the product.\n",
    "  products.product_name: name of the product.\n",
    "  products.product_brand: the brand under which the product was presented.\n",
    "  products.product_manufacturer: product manufacturer.\n",
    "  products.product_company_name: company owning the product.\n",
    "  products.product_price: price of the product at crawling time.\n",
    "  products.product_average-rating: average ratings across all feedbacks for the product, at crawling time.\n",
    "\n",
    "  Table company -> column company_name relates to table feedback -> column product_company_name\n",
    "  Table products -> column product_company_name relates to table feedback -> column product_company-name\n",
    "  Table feedback -> column product_id relates to table products -> column product_id\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977899ef",
   "metadata": {},
   "source": [
    "### Define state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d272a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the state of the graph, which includes user's question, AI's answer, query that has been created and its result;\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, RemoveMessage\n",
    "\n",
    "class State(TypedDict):\n",
    " messages_log: Sequence[BaseMessage]\n",
    " current_question: str\n",
    " log_sql_queries: list[dict]\n",
    " current_sql_queries: list[dict]\n",
    " llm_answer: BaseMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27d37d",
   "metadata": {},
   "source": [
    "### Create sql query or queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2a830ba6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# create a function that generates the sql query to be executed\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
    "\n",
    "class OutputAsAQuery(TypedDict):\n",
    "  \"\"\" generated sql query or sql queries if there are multiple \"\"\"\n",
    "  query: Annotated[list[str],\"clean sql query\"]\n",
    "\n",
    "def create_sql_query_or_queries(state:State):\n",
    "  \"\"\" creates a sql query based on the question \"\"\"\n",
    "\n",
    "  system_prompt = \"\"\"You are a sql expert and an expert data modeler.  \n",
    "\n",
    "  Your task is to create a sql script to answer the user's question. In the sql script, use only these tables and columns you have access to:\n",
    "  {objects_documentation}\n",
    "\n",
    "  User question:\n",
    "  {question}\n",
    "\n",
    "  Answer just with the resulting sql code.\n",
    "\n",
    "  IMPORTANT:\n",
    "    - Return only raw SQL strings in the list.\n",
    "    - DO NOT include comments (like \"-- Query 1\"), labels, or explanations.\n",
    "    - If only one SQL query is needed, just return a list with that one query.\n",
    "    - Do not generate more than 5 queries.\n",
    "\n",
    "  Example output:\n",
    "    [\n",
    "      \"SELECT COUNT(*) FROM feedback;\",\n",
    "      \"SELECT AVG(product_price) FROM products;\"\n",
    "    ]\n",
    "  \"\"\"\n",
    "\n",
    "  prompt = ChatPromptTemplate.from_messages(\n",
    "    ('system', system_prompt)\n",
    "  )\n",
    "\n",
    "  chain = prompt | llm.with_structured_output(OutputAsAQuery)\n",
    "\n",
    "  result = chain.invoke({'objects_documentation':objects_documentation, 'question': state['current_question']})\n",
    "  for q in result['query']:\n",
    "   state['current_sql_queries'].append( {'query': q,\n",
    "                                     'result':'', ## add it later\n",
    "                                     'insight': '', ## add it later\n",
    "                                     'metadata':'' ## add it later\n",
    "                                      } )\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "188b45f9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# since gpt-4o allows a maximum completion limit (output context limit) of 4k tokens, I half it to get maximum context size, so 2k. Assuming the entire context is not just the data,\n",
    "# I divide this number by 5 and arrive at a limit of 400 tokens for the result of the sql query.\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "maximum_nr_tokens_sql_query = 200\n",
    "\n",
    "# create a function that counts the tokens from a string\n",
    "def count_tokens(string:str):\n",
    " \"\"\" returns the number of tokens in a text string \"\"\"\n",
    " encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    " num_tokens = len(encoding.encode(string))\n",
    " return num_tokens\n",
    "\n",
    "# create a function that compares the tokens from the sql query result with the maximum token limit, and returns true if the context limit has been exceeded, false otherwise.\n",
    "def check_if_exceed_maximum_context_limit(sql_query_result):\n",
    " \"\"\" compares the tokens from the sql query result with the maximum token limit, and returns true if the context limit has been exceeded, false otherwise \"\"\"\n",
    " tokens_sql_query_result = count_tokens(sql_query_result)\n",
    " if tokens_sql_query_result > maximum_nr_tokens_sql_query:\n",
    "  return True\n",
    " else:\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0992424",
   "metadata": {},
   "source": [
    "### Create sql query insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "0c2613a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# create a function that explains the limit of a sql query\n",
    "\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "def create_sql_query_limitation(sql_query:str):\n",
    " \"\"\" creates a concise explanation of the sql query by pointing out its limitations \"\"\"\n",
    "\n",
    " system_prompt = \"\"\"\n",
    "You are an expert data analyst.\n",
    "\n",
    "You are provided with the following SQL query:\n",
    "{sql_query}\n",
    "\n",
    "Your task is to identify **inherent limitations or assumptions** of the query **based strictly on its structure and logic**.\n",
    "\n",
    "Focus only on:\n",
    "- how LIMIT, ORDER BY, GROUP BY, or JOINs may introduce assumptions\n",
    "- how filtering or aggregation logic may bias the output\n",
    "- situations where the query might **return incomplete or misleading results due to logic only**\n",
    "- cases where ORDER BY combined with LIMIT might exclude other rows with equal values (ties)\n",
    "\n",
    "Only describe things that follow **logically from the query**, not from the dataset itself.\n",
    "\n",
    "🚫 Do NOT mention:\n",
    "- speculate on what the user is trying to analyze\n",
    "- suggest what insights are missing\n",
    "- mention field names being correct or incorrect\n",
    "- mention data types, nulls, formatting, spelling, or schema correctness\n",
    "- mention what other attributes, columns, filters, or relationships \"could have\" been used\n",
    "- assume anything about the intent behind the query\n",
    "\n",
    "👉 If the query has **no structural limitations or assumptions**, respond with exactly:\n",
    "No comments for the query.\n",
    "\n",
    "Respond in 1 to 3 concise sentences, or with the exact phrase above.\n",
    " \"\"\"\n",
    "\n",
    " prompt = ChatPromptTemplate.from_messages(\n",
    "     ('system',system_prompt)\n",
    " )\n",
    "\n",
    " chain = prompt | llm | StrOutputParser()\n",
    " return chain.invoke({'sql_query':sql_query})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a7489351",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class OutputAsASingleQuery(TypedDict):\n",
    "  \"\"\" generated sql query \"\"\"\n",
    "  query: Annotated[str,...,\"the generated sql query\"]\n",
    "\n",
    "def refine_sql_query(question: str, sql_query: str, maximum_nr_tokens_sql_query: int):\n",
    " \"\"\" refines the sql query so that its output tokens do not exceed the maximum context limit \"\"\"\n",
    "\n",
    " system_prompt = \"\"\"\n",
    " You are a sql expert an an expert data modeler.\n",
    "\n",
    " You are tying to answer the following question from the user:\n",
    " {question}\n",
    "\n",
    " The following sql query produces an output that exceeds {maximum_nr_tokens_sql_query} tokens:\n",
    " {sql_query}\n",
    "\n",
    " Please optimize this query so that its output stays within the token limit while still providing as much insight as possible to answer the question.\n",
    " Prefer using WHERE or LIMIT clauses to reduce the size of the result.\n",
    " \"\"\"\n",
    " \n",
    " prompt = ChatPromptTemplate.from_messages(\n",
    "   ('system',system_prompt)\n",
    " )\n",
    "\n",
    " chain = (prompt\n",
    "         | llm.with_structured_output(OutputAsASingleQuery)\n",
    " )\n",
    "\n",
    " sql_query = chain.invoke({'question': question,\n",
    "               'sql_query':sql_query,\n",
    "               'maximum_nr_tokens_sql_query':maximum_nr_tokens_sql_query}\n",
    "               )\n",
    " return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "c2447ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that creates an explanation of a sql query\n",
    "\n",
    "def create_sql_query_insight(sql_query:str, sql_query_result:str, sql_query_limitation:str):\n",
    " \"\"\" creates an insight from the result of the sql query \"\"\"\n",
    "\n",
    " system_prompt = \"\"\"\n",
    "Create an insight as a result of running the following query:\n",
    "\n",
    "Sql query:\n",
    "{sql_query}\n",
    "\n",
    "Which yielded the following result:\n",
    "{sql_query_result}\n",
    "\n",
    "Having this limitation:\n",
    "{sql_query_limitation}\n",
    "\n",
    "Do not mention your subjective assessment over the results.\n",
    "Avoid technical terms like \"data\",\"dataset\",\"table\",\"list\",\"provided information\",\"query\" etc.\n",
    "Focus on stating the fact or insight directly and clearly.\n",
    " \"\"\"\n",
    "\n",
    " prompt = ChatPromptTemplate.from_messages(\n",
    "     ('system',system_prompt)\n",
    " )\n",
    "\n",
    " chain = prompt | llm | StrOutputParser()\n",
    " return chain.invoke({'sql_query':sql_query, 'sql_query_result': sql_query_result, 'sql_query_limitation':sql_query_limitation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc11ef",
   "metadata": {},
   "source": [
    "### Create sql query metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "231961b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "from sqlglot import parse_one, exp\n",
    "\n",
    "def extract_metadata_from_sql_query(sql_query):\n",
    "   # returns a dictionary with parsed names of tables and columns used in filters, aggregations and groupings \n",
    "   \n",
    " ast = parse_one(sql_query)\n",
    "\n",
    " sql_query_metadata = {\n",
    "    \"tables\": [],\n",
    "    \"filters\": [],\n",
    "    \"aggregations\": [],\n",
    "    \"groupings\": []\n",
    " }\n",
    "\n",
    " # extract tables\n",
    " table_generator = ast.find_all(sqlglot.expressions.Table)\n",
    " for items in table_generator:\n",
    "    sql_query_metadata['tables'].append(items.sql())\n",
    " # remove dups\n",
    " sql_query_metadata['tables'] = list(dict.fromkeys(sql_query_metadata['tables']))\n",
    "\n",
    " # extract filters\n",
    " where_conditions = ast.find_all(sqlglot.expressions.Where)\n",
    " for item in where_conditions:\n",
    "  sql_query_metadata['filters'].append(item.this.sql())\n",
    "  # remove dups\n",
    " sql_query_metadata['filters'] = list(dict.fromkeys(sql_query_metadata['filters']))\n",
    "\n",
    " # extract aggregate functions\n",
    " funcs = ast.find_all(sqlglot.expressions.AggFunc)\n",
    " for item in funcs:\n",
    "  sql_query_metadata['aggregations'].append(item.sql())\n",
    " # remove dups\n",
    " sql_query_metadata['aggregations'] = list(dict.fromkeys(sql_query_metadata['aggregations']))\n",
    "\n",
    " # extract groupings\n",
    " groupings = ast.find_all(sqlglot.expressions.Group)\n",
    " for item in groupings:\n",
    "  groupings_flattened = item.flatten()\n",
    "  for item in groupings_flattened:\n",
    "    sql_query_metadata['groupings'].append(item.sql())\n",
    " # remove dups\n",
    " sql_query_metadata['groupings'] = list(dict.fromkeys(sql_query_metadata['groupings']))\n",
    "\n",
    " return {'tables':sql_query_metadata.get('tables'),\n",
    "         'filters':sql_query_metadata.get('filters'),\n",
    "         'aggregations':sql_query_metadata.get('aggregations'),\n",
    "         'groupings':sql_query_metadata.get('groupings'),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d6f7516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sql_metadata_explanation(tables:list, filters:list, aggregations:list, groupings:list,header :str='') -> str:\n",
    "    # creates a string explanation of the filters, tables, aggregations and groupings used by the query\n",
    "    explanation = header\n",
    "\n",
    "    if tables:\n",
    "        explanation += \"\\n\\n🧊 Tables: • \" + \" • \".join(tables)\n",
    "    if filters:\n",
    "        explanation += \"\\n\\n🔍 Filters: • \" + \" • \".join(filters)\n",
    "    if aggregations:\n",
    "        explanation += \"\\n\\n🧮 Aggregations: • \" + \" • \".join(aggregations)\n",
    "    if groupings:\n",
    "        explanation += \"\\n\\n📦 Groupings: • \" + \" • \".join(groupings)\n",
    "\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ad2f3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_metadata(sql_query: str):\n",
    " \"\"\" creates an explanation for one single query \"\"\"\n",
    "\n",
    " metadata = extract_metadata_from_sql_query(sql_query)\n",
    " return format_sql_metadata_explanation(metadata['tables'],metadata['filters'],metadata['aggregations'],metadata['groupings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d3a2ff8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_queries_metadata(sql_queries: list[dict]):\n",
    " \"\"\" creates an explanation for multiple queries \"\"\"\n",
    "\n",
    " all_tables = []\n",
    " all_filters = []\n",
    " all_aggregations = []\n",
    " all_groupings = []\n",
    "\n",
    " for q in sql_queries: \n",
    "\n",
    "  metadata = extract_metadata_from_sql_query(q['query'])\n",
    "  all_tables.extend(metadata[\"tables\"])\n",
    "  all_filters.extend(metadata[\"filters\"])\n",
    "  all_aggregations.extend(metadata[\"aggregations\"])\n",
    "  all_groupings.extend(metadata[\"groupings\"])\n",
    "\n",
    " return format_sql_metadata_explanation(all_tables,all_filters,all_aggregations,all_groupings,header='I analyzed data based on the following filters and transformations:')\n",
    "\n",
    "# use it like so:\n",
    "#sql_queries = [ \n",
    "#    {'query':'SELECT COUNT(DISTINCT company.company_name) FROM company;', 'result':''} ,\n",
    "#    {'query':'SELECT COUNT(DISTINCT feedback.feedback_id) FROM feedback;', 'result':''} \n",
    "#    ]\n",
    "#create_queries_metadata(sql_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1c989",
   "metadata": {},
   "source": [
    "### Execute sql query and stores result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f766366d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# the function checks if the query output exceeds context window limit and if yes, send the query for refinement\n",
    "\n",
    "from langchain_community.tools import QuerySQLDataBaseTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from typing import Iterator\n",
    "\n",
    "db = SQLDatabase(engine)\n",
    "\n",
    "def execute_sql_query(state:State):\n",
    "  \"\"\" executes the sql query and retrieve the result \"\"\"\n",
    "\n",
    "  for query_index, q in enumerate(state['current_sql_queries']):\n",
    "     \n",
    "    sql_query = q['query'] \n",
    "    \n",
    "    # refine the query 3 times if necessary.\n",
    "    for i in range(3):\n",
    "\n",
    "      sql_query_result = QuerySQLDataBaseTool(db=db).invoke(sql_query)\n",
    "\n",
    "      # if the sql query does not exceed output context window return its result\n",
    "      if not check_if_exceed_maximum_context_limit(sql_query_result):\n",
    "\n",
    "       sql_query_limitation = create_sql_query_limitation(sql_query)\n",
    "       sql_query_insight = create_sql_query_insight(sql_query,sql_query_result, sql_query_limitation)\n",
    "       sql_query_metadata = create_query_metadata(sql_query)\n",
    "\n",
    "       state['current_sql_queries'][query_index]['result'] = sql_query_result\n",
    "       state['current_sql_queries'][query_index]['insight'] = sql_query_insight\n",
    "       state['current_sql_queries'][query_index]['query'] = sql_query\n",
    "       state['current_sql_queries'][query_index]['metadata'] = sql_query_metadata\n",
    "\n",
    "       # add the queries to the logs \n",
    "       state['log_sql_queries'].append({'query': sql_query,\n",
    "                                     'result':sql_query_result, \n",
    "                                     'insight': sql_query_insight, \n",
    "                                     'metadata':sql_query_metadata \n",
    "                                      })   \n",
    "       break\n",
    "\n",
    "      # if the sql query exceeds output context window and there is more room for iterations, refine the query\n",
    "      else:\n",
    "        print(f\"🔧 Refining query {query_index+1}/{len(state['current_sql_queries'])} as its output its too large...\")\n",
    "        sql_query = refine_sql_query(state['current_question'],sql_query,maximum_nr_tokens_sql_query)['query']\n",
    "\n",
    "      # if there is no more room for sql query iterations and the result still exceeds context window, throw a message\n",
    "\n",
    "    else:\n",
    "      print(f\"⚠️ Query result too large after 3 refinements.\")\n",
    "      state['current_sql_queries'][query_index]['result'] = 'Query result too large after 3 refinements.'\n",
    "      state['current_sql_queries'][query_index]['explanation'] = \"Refinement failed.\"\n",
    "      \n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952f76b",
   "metadata": {},
   "source": [
    "### Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "65eaed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sql_query_results_for_prompt (sql_queries : list[dict]) -> str:\n",
    "    \n",
    "    formatted_queries = []\n",
    "    for query_index,q in enumerate(sql_queries):\n",
    "        block = f\"Insight {query_index+1}:\\n{q['insight']}\\n\\nRaw Result of insight {query_index+1}:\\n{q['result']}\"\n",
    "        formatted_queries.append(block)\n",
    "    return \"\\n\\n\".join(formatted_queries)\n",
    "\n",
    "# print(format_sql_query_results_for_prompt(test_state['sql_queries']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "17735fb5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## create a function that generates the agent answer based on sql query result\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def generate_answer(state:State):\n",
    "  \"\"\" generates the AI answer taking into consideration the explanation and the result of the sql query that was executed \"\"\"\n",
    "\n",
    "  system_prompt = \"\"\" You are a decision support consultant helping users become more data-driven.\n",
    "     Continue the conversation by answering the following question: {question}.\n",
    "\n",
    "     Use both the raw SQL results and the extracted insights below to form your answer: {insights}. Include all details from these insights.\n",
    "\n",
    "     Respond in clear, concise, non-technical language.\n",
    "     \"\"\"\n",
    "\n",
    "  prompt = ChatPromptTemplate.from_messages([\n",
    "      MessagesPlaceholder(\"messages_log\") ,\n",
    "      ('system',system_prompt)            \n",
    "  ] )\n",
    "\n",
    "  llm_answer_chain = prompt | llm \n",
    "  final_answer_chain = { 'llm_answer': llm_answer_chain, 'input_state': RunnablePassthrough() } | RunnableLambda (lambda x: { 'ai_message': AIMessage( content = f\"{x['llm_answer'].content}\\n\\n{create_queries_metadata(x['input_state']['current_sql_queries'])}\", \n",
    "                                                                                                                                                        response_metadata = x['llm_answer'].response_metadata  ) } ) \n",
    "\n",
    "  result = final_answer_chain.invoke({ 'messages_log':state['messages_log'],\n",
    "               'question':state['current_question'],\n",
    "               'insights': format_sql_query_results_for_prompt(state['current_sql_queries']),\n",
    "              'current_sql_queries': state['current_sql_queries'] })\n",
    "  \n",
    "  ai_msg = result['ai_message']\n",
    "\n",
    "  explanation_token_count = llm.get_num_tokens(create_queries_metadata(state['current_sql_queries']))\n",
    "  ai_msg.response_metadata['token_usage']['total_tokens'] += explanation_token_count\n",
    "\n",
    "  state['llm_answer'] = ai_msg\n",
    "  state['messages_log'].append(HumanMessage(state['current_question']))\n",
    "  state['messages_log'].append(ai_msg)\n",
    "\n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68cae6",
   "metadata": {},
   "source": [
    "### Manage memory and chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "6f942bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_memory_chat_history(state:State):\n",
    "    \"\"\" Manages the chat history so that it does not become too large in terms of output tokens.\n",
    "    Specifically, it checks if the chat history is larger than 1000 tokens. If yes, keep just the last 4 pairs of human prompts and AI responses, and summarize the older messages.\n",
    "    Additionally, check if the logs of sql queries is larger than 20 entries. If yes, delete the older records. \"\"\"           \n",
    "\n",
    "    tokens_chat_history = state['messages_log'][-1].response_metadata.get('token_usage', {}).get('total_tokens', 0) if state['messages_log'] else 0\n",
    "    \n",
    "\n",
    "    if tokens_chat_history >= 1000 and len(state['messages_log']) > 4:\n",
    "        message_history_to_summarize = [msg.content for msg in state['messages_log'][:-4]]\n",
    "        prompt = ChatPromptTemplate.from_messages( [('user', 'Distill the below chat messages into a single summary paragraph.The summary paragraph should have maximum 400 tokens.Include as many specific details as you can.Chat messages:{message_history_to_summarize}') ])\n",
    "        runnable = prompt | llm\n",
    "        chat_history_summary = runnable.invoke({'message_history_to_summarize':message_history_to_summarize})\n",
    "        last_4_messages = state['messages_log'][-4:]\n",
    "        state['messages_log'] = [chat_history_summary] +[*last_4_messages]\n",
    "    else:\n",
    "        state['messages_log'] = state['messages_log']\n",
    "\n",
    "    # Truncate SQL logs to the most recent 20\n",
    "    if len(state['log_sql_queries']) > 20:\n",
    "        state['log_sql_queries']= state['log_sql_queries'][-20:]    \n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7ce8d",
   "metadata": {},
   "source": [
    "### Assemble graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "23aa439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble graph\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# function to reset the state current queries (to add in the start of graph execution)\n",
    "def reset_current_sql_queries(state:State):\n",
    "    state['current_sql_queries'] = []\n",
    "    return state\n",
    "\n",
    "graph= StateGraph(State)\n",
    "graph.add_node(\"reset_current_sql_queries\",reset_current_sql_queries)\n",
    "graph.add_node(\"create_sql_query_or_queries\",create_sql_query_or_queries)\n",
    "graph.add_node(\"execute_sql_query\",execute_sql_query)\n",
    "graph.add_node(\"generate_answer\",generate_answer)\n",
    "graph.add_node(\"manage_memory_chat_history\",manage_memory_chat_history)\n",
    "\n",
    "graph.add_edge(START,\"reset_current_sql_queries\")\n",
    "graph.add_edge(\"reset_current_sql_queries\",\"create_sql_query_or_queries\")\n",
    "graph.add_edge(\"create_sql_query_or_queries\",\"execute_sql_query\")\n",
    "graph.add_edge(\"execute_sql_query\",\"generate_answer\")\n",
    "graph.add_edge(\"generate_answer\",\"manage_memory_chat_history\")\n",
    "graph.add_edge(\"manage_memory_chat_history\",END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcd929",
   "metadata": {},
   "source": [
    "\n",
    "### test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new conversation\n",
    "\n",
    "question = 'What is the product with the highest ranking?'\n",
    "messages_log = []\n",
    "\n",
    "initial_dict = {'objects_documentation':objects_documentation,\n",
    "     'messages_log': messages_log,\n",
    "     'current_question':question,\n",
    "     'log_sql_queries': [],\n",
    "     'current_sql_queries': [],\n",
    "     'llm_answer': []\n",
    "     }\n",
    "\n",
    "config, thread_id = create_config('Run Agent',True)\n",
    "if __name__ == \"__main__\":\n",
    " for step in graph.stream(initial_dict, config = config, stream_mode=\"updates\"):\n",
    "   step_name, output = list(step.items())[0]\n",
    "   if step_name == 'create_sql_query_or_queries':\n",
    "    print(f\"✅ SQL queries created:{len(output['current_sql_queries'])}\")\n",
    "   elif step_name == 'execute_sql_query':\n",
    "    print(\"⚙️ Analysing results...\")\n",
    "   elif step_name == 'generate_answer':\n",
    "    print(\"\\n📣 Final Answer:\\n\")\n",
    "    print(output['llm_answer'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "\n",
    "initial_dict['current_question'] = 'How many products have this rating?' \n",
    "\n",
    "\n",
    "config, _ = create_config('Run Agent',False,thread_id)\n",
    "if __name__ == \"__main__\":\n",
    " for step in graph.stream(initial_dict, config = config, stream_mode=\"updates\"):\n",
    "   step_name, output = list(step.items())[0]\n",
    "   if step_name == 'create_sql_query_or_queries':\n",
    "    print(f\"✅ SQL queries created:{len(output['current_sql_queries'])}\")\n",
    "   elif step_name == 'execute_sql_query':\n",
    "    print(\"⚙️ Analysing results...\")\n",
    "   elif step_name == 'generate_answer':\n",
    "    print(\"\\n📣 Final Answer:\\n\")\n",
    "    print(output['llm_answer'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ffa2c",
   "metadata": {},
   "source": [
    "### Testing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is the product with the highest ranking?'\n",
    "\n",
    "test_state = {'messages_log':[],\n",
    "'current_question':question,\n",
    "'log_sql_queries': [],\n",
    "'current_sql_queries': [],\n",
    "'llm_answer': []\n",
    "}\n",
    "create_sql_query_or_queries(test_state)\n",
    "execute_sql_query(test_state)\n",
    "generate_answer(test_state)\n",
    "manage_memory_chat_history(test_state)\n",
    "test_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b9ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful pieces of code\n",
    "\n",
    "# graph.get_state(config).values"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "951c4e1ccc8e7dc066b7b3456b4d29f8a6c8c8949bd81a565897b5da2568416e"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
